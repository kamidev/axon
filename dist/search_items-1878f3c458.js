searchNodes=[{"doc":"A high-level interface for creating neural network models. Axon is built entirely on top of Nx numerical definitions, so every neural network can be JIT or AOT compiled using any Nx compiler, or even transformed into high-level neural network formats like TensorFlow Lite and ONNX . Model Creation All Axon models start with an input layer, specifying the expected input shape of the training data: input = Axon . input ( { nil , 784 } , &quot;input&quot; ) Notice you can specify some dimensions as nil , indicating that the dimension size will be filled in at model runtime. You can then compose inputs with other layers: model = input |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.8 ) |&gt; Axon . dense ( 64 ) |&gt; Axon . tanh ( ) |&gt; Axon . dense ( 10 ) |&gt; Axon . activation ( :softmax ) You can inspect the model for a nice summary: IO . inspect ( model ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Model === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === Layer Shape Policy Parameters Parameters Memory === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === === input ( input ) { nil , 784 } p = f32 c = f32 o = f32 0 0 bytes dense_0 ( dense [ &quot;input&quot; ] ) { nil , 128 } p = f32 c = f32 o = f32 100480 401920 bytes relu_0 ( relu [ &quot;dense_0&quot; ] ) { nil , 128 } p = f32 c = f32 o = f32 0 0 bytes batch_norm_0 ( batch_norm [ &quot;relu_0&quot; ] ) { nil , 128 } p = f32 c = f32 o = f32 512 2048 bytes dropout_0 ( dropout [ &quot;batch_norm_0&quot; ] ) { nil , 128 } p = f32 c = f32 o = f32 0 0 bytes dense_1 ( dense [ &quot;dropout_0&quot; ] ) { nil , 64 } p = f32 c = f32 o = f32 8256 33024 bytes tanh_0 ( tanh [ &quot;dense_1&quot; ] ) { nil , 64 } p = f32 c = f32 o = f32 0 0 bytes dense_2 ( dense [ &quot;tanh_0&quot; ] ) { nil , 10 } p = f32 c = f32 o = f32 650 2600 bytes softmax_0 ( softmax [ &quot;dense_2&quot; ] ) { nil , 10 } p = f32 c = f32 o = f32 0 0 bytes -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Total Parameters : 109898 Total Parameters Memory : 439592 bytes Inputs : %{ &quot;input&quot; =&gt; { nil , 784 } } Multiple Inputs Creating a model with multiple inputs is as easy as declaring an additional input in your Axon graph. Every input layer present in the final Axon graph will be required to be passed as input at the time of model execution. inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) inp2 = Axon . input ( { nil , 1 } , &quot;input_1&quot; ) # Both inputs will be used model1 = Axon . add ( inp1 , inp2 ) # Only inp2 will be used model2 = Axon . add ( inp2 , inp2 ) Axon graphs are immutable, which means composing and manipulating an Axon graph creates an entirely new graph. Additionally, layer names are lazily generated at model execution time. To avoid non-deterministic input orderings and names, Axon requires each input to have a unique binary identifier. You can then reference inputs by name when passing to models at execution time: inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) inp2 = Axon . input ( { nil , 1 } , &quot;input_1&quot; ) model1 = Axon . add ( inp1 , inp2 ) params1 = Axon . init ( model1 ) # Inputs are referenced by name Axon . predict ( model1 , params1 , %{ &quot;input_0&quot; =&gt; x , &quot;input_1&quot; =&gt; y } ) Multiple Outputs Nx offers robust container support which is extended to Axon. Axon allows you to wrap any valid Nx container in a layer. Containers are most commonly used to structure outputs: inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) inp2 = Axon . input ( { nil , 1 } , &quot;input_1&quot; ) model = Axon . container ( %{ foo : inp1 , bar : inp2 } ) Containers can be arbitrarily nested: inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) inp2 = Axon . input ( { nil , 1 } , &quot;input_1&quot; ) model = Axon . container ( { %{ foo : { inp1 , %{ bar : inp2 } } } } ) You can even use custom structs which implement the container protocol: inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) inp2 = Axon . input ( { nil , 1 } , &quot;input_1&quot; ) model = Axon . container ( % MyStruct { foo : inp1 , bar : inp2 } ) Custom Layers If you find that Axon's built-in layers are insufficient for your needs, you can create your own using the custom layer API. All of Axon's built-in layers (aside from special ones such as input , constant , and container ) make use of this same API. Axon layers are really just placeholders for Nx computations with trainable parameters and possibly state. To define a custom layer, you just need to define a defn implementation: defn my_layer ( x , weight , _opts \\ [ ] ) do Nx . atan2 ( x , weight ) end Notice the only stipulation is that your custom layer implementation must accept at least 1 input and a list of options. At execution time, every layer will be passed a :mode option which can be used to control behavior at training and inference time. Inputs to your custom layer can be either Axon graph inputs or trainable parameters. You can pass Axon graph inputs as-is to a custom layer. To declare trainable parameters, use Axon.param/3 : weight = Axon . param ( input_shape , &quot;weight&quot; ) To create a custom layer, you &quot;wrap&quot; your implementation and inputs into a layer using Axon.layer . You'll notice the API mirrors Elixir's apply : def atan2_layer ( % Axon { output_shape : shape } = input ) do weight = Axon . param ( input_shape , &quot;weight&quot; ) Axon . layer ( &amp; my_layer / 3 , [ input , weight ] ) end Model Execution Under the hood, Axon models are represented as Elixir structs. You can initialize and apply models using the macros Axon.init/3 and Axon.predict/4 : params = Axon . init ( model , compiler : EXLA ) Axon . predict ( model , params , inputs , compiler : EXLA , mode : :train ) It is suggested that you set compiler options globally rather than pass them as options to execution macros: EXLA . set_as_nx_default ( [ :tpu , :cuda , :rocm , :host ] ) params = Axon . init ( model ) Axon . predict ( model , params , inputs , mode : :train ) Axon.predict/4 by default runs in inference mode, which performs certain optimizations and removes layers such as dropout layers. If constructing a training step using Axon.predict/4 , be sure to specify mode: :train . Model Training Combining the Axon model creation API with the optimization and training APIs, you can create and train neural networks with ease: model = Axon . input ( { nil , 784 } , &quot;input_0&quot; ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . layer_norm ( ) |&gt; Axon . dropout ( ) |&gt; Axon . dense ( 10 , activation : :softmax ) IO . inspect model model_state = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adamw ( 0.005 ) ) |&gt; Axon.Loop . run ( train_data , epochs : 10 , compiler : EXLA ) See Axon.Updates and Axon.Loop for a more in-depth treatment of model optimization and model training.","ref":"Axon.html","title":"Axon","type":"module"},{"doc":"Adds an activation layer to the network. Activation layers are element-wise functions typically called after the output of another layer. Options :name - layer name.","ref":"Axon.html#activation/3","title":"Axon.activation/3","type":"function"},{"doc":"Adds an Adaptive average pool layer to the network. See Axon.Layers.adaptive_avg_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#adaptive_avg_pool/2","title":"Axon.adaptive_avg_pool/2","type":"function"},{"doc":"Adds an Adaptive power average pool layer to the network. See Axon.Layers.adaptive_lp_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#adaptive_lp_pool/2","title":"Axon.adaptive_lp_pool/2","type":"function"},{"doc":"Adds an Adaptive max pool layer to the network. See Axon.Layers.adaptive_max_pool/2 for more details. Options :name - layer name. :output_size - layer output size. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#adaptive_max_pool/2","title":"Axon.adaptive_max_pool/2","type":"function"},{"doc":"Adds a add layer to the network. This layer performs an element-wise add operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#add/3","title":"Axon.add/3","type":"function"},{"doc":"Adds an Alpha dropout layer to the network. See Axon.Layers.alpha_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 .","ref":"Axon.html#alpha_dropout/2","title":"Axon.alpha_dropout/2","type":"function"},{"doc":"Attaches a hook to the given Axon model. Hooks compile down to Nx.Defn.Kernel.hook/3 and provide the same functionality for adding side-effecting operations to a compiled model. For example, you can use hooks to inspect intermediate activations, send data to an external service, and more. Hooks can be configured to be invoked on the following events: :initialize - on model initialization. :pre_forward - before layer forward pass is invoked. :forward - after layer forward pass is invoked. :backward - after layer backward pass is invoked. To invoke a hook on every single event, you may pass :all to on: . Axon . input ( { nil , 1 } , &quot;input&quot; ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :all ) The default event is :forward , assuming you want a hook invoked on the layers forward pass. You may configure hooks to run in one of only training or inference mode using the :mode option. The default mode is :both to be invoked during both train and inference mode. Axon . input ( { nil , 1 } , &quot;input&quot; ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 , on : :forward , mode : :train ) You can also attach multiple hooks to a single layer. Hooks are invoked in the order in which they are declared. If order is important, you should attach hooks in the order you want them to be executed: Axon . input ( { nil , 1 } , &quot;input&quot; ) # I will be executed first |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 ) # I will be executed second |&gt; Axon . attach_hook ( fn _ -&gt; IO . write ( &quot;HERE&quot; ) end ) Hooks are executed at their point of attachment. You must insert hooks at each point you want a hook to execute during model execution. Axon . input ( { nil , 1 } , &quot;input&quot; ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 ) |&gt; Axon . relu ( ) |&gt; Axon . attach_hook ( &amp; IO . inspect / 1 )","ref":"Axon.html#attach_hook/3","title":"Axon.attach_hook/3","type":"function"},{"doc":"Adds an Average pool layer to the network. See Axon.Layers.avg_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#avg_pool/2","title":"Axon.avg_pool/2","type":"function"},{"doc":"Adds a Batch normalization layer to the network. See Axon.Layers.batch_norm/6 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to 1 . :epsilon - numerical stability term.","ref":"Axon.html#batch_norm/2","title":"Axon.batch_norm/2","type":"function"},{"doc":"Adds a bias layer to the network. A bias layer simply adds a trainable bias to an input. Options :name - layer name. :bias_initializer - initializer for bias weights. Defaults to :zeros .","ref":"Axon.html#bias/2","title":"Axon.bias/2","type":"function"},{"doc":"Adds a bilinear layer to the network. The bilinear layer implements: output = activation ( dot ( dot ( input1 , kernel ) , input2 ) + bias ) where activation is given by the :activation option and both kernel and bias are layer parameters. units specifies the number of output units. All dimensions but the last of input1 and input2 must match. The batch sizes of both inputs must also match or at least one must be nil . Inferred output batch size coerces to the strictest input batch size. Compiles to Axon.Layers.bilinear/5 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#bilinear/4","title":"Axon.bilinear/4","type":"function"},{"doc":"Adds a Continuously-differentiable exponential linear unit activation layer to the network. See Axon.Activations.celu/1 for more details. Options :name - layer name.","ref":"Axon.html#celu/2","title":"Axon.celu/2","type":"function"},{"doc":"Compiles the given model to {init_fn, predict_fn} . Once compiled, a model can be passed as argument to Nx.Defn .","ref":"Axon.html#compile/2","title":"Axon.compile/2","type":"function"},{"doc":"Adds a concatenate layer to the network. This layer will concatenate inputs along the last dimension unless specified otherwise. Options :name - layer name. :axis - concatenate axis. Defaults to -1 .","ref":"Axon.html#concatenate/3","title":"Axon.concatenate/3","type":"function"},{"doc":"Adds a conditional layer which conditionally executes true_graph or false_graph based on the condition cond_fn at runtime. cond_fn is an arity-1 function executed on the output of the parent graph. It must return a boolean scalar tensor (e.g. 1 or 0). The shapes of true_graph and false_graph must be equal.","ref":"Axon.html#cond/5","title":"Axon.cond/5","type":"function"},{"doc":"Adds a constant layer to the network. Constant layers encapsulate Nx tensors in an Axon layer for ease of use with other Axon layers. They can be used interchangeably with other Axon layers: inp = Axon . input ( { nil , 32 } , &quot;input&quot; ) my_constant = Axon . constant ( Nx . iota ( { 1 , 32 } ) ) model = Axon . add ( inp , my_constant ) Constant layers will be cast according to the mixed precision policy. If it's important for your constant to retain it's type during the computation, you will need to set the mixed precision policy to ignore constant layers. Options :name - layer name.","ref":"Axon.html#constant/2","title":"Axon.constant/2","type":"function"},{"doc":"Adds a container layer to the network. In certain cases you may want your model to have multiple outputs. In order to make this work, you must &quot;join&quot; the outputs into an Axon layer using this function for use in initialization and inference later on. The given container can be any valid Axon Nx container. Options :name - layer name. Examples iex&gt; inp1 = Axon . input ( { nil , 1 } , &quot;input_0&quot; ) iex&gt; inp2 = Axon . input ( { nil , 2 } , &quot;input_1&quot; ) iex&gt; model = Axon . container ( %{ a : inp1 , b : inp2 } ) iex&gt; %{ a : a , b : b } = Axon . predict ( model , %{ } , %{ ...&gt; &quot;input_0&quot; =&gt; Nx . tensor ( [ [ 1.0 ] ] ) , ...&gt; &quot;input_1&quot; =&gt; Nx . tensor ( [ [ 1.0 , 2.0 ] ] ) ...&gt; } ) iex&gt; a # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 1.0 ] ] &gt; iex&gt; b # Nx.Tensor &lt; f32 [ 1 ] [ 2 ] [ [ 1.0 , 2.0 ] ] &gt;","ref":"Axon.html#container/2","title":"Axon.container/2","type":"function"},{"doc":"Adds a convolution layer to the network. The convolution layer implements a general dimensional convolutional layer - which convolves a kernel over the input to produce an output. Compiles to Axon.Layers.conv/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :feature_group_size - feature group size for convolution. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#conv/3","title":"Axon.conv/3","type":"function"},{"doc":"See conv_lstm/3 .","ref":"Axon.html#conv_lstm/2","title":"Axon.conv_lstm/2","type":"function"},{"doc":"Adds a convolutional long short-term memory (LSTM) layer to the network with a random initial hidden state. See conv_lstm/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :glorot_uniform .","ref":"Axon.html#conv_lstm/3","title":"Axon.conv_lstm/3","type":"function"},{"doc":"Adds a convolutional long short-term memory (LSTM) layer to the network with the given initial hidden state.. ConvLSTMs apply Axon.Recurrent.conv_lstm_cell/5 over an entire input sequence and return: { { new_cell , new_hidden } , output_sequence } You can use the output state as the hidden state of another ConvLSTM layer. Options :name - layer name. :padding - convolutional padding. Defaults to :same . :kernel_size - convolutional kernel size. Defaults to 1 . :strides - convolutional strides. Defaults to 1 . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#conv_lstm/4","title":"Axon.conv_lstm/4","type":"function"},{"doc":"Adds a transposed convolution layer to the network. The transposed convolution layer is sometimes referred to as a fractionally strided convolution or (incorrectly) as a deconvolution. Compiles to Axon.Layers.conv_transpose/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#conv_transpose/3","title":"Axon.conv_transpose/3","type":"function"},{"doc":"","ref":"Axon.html#deep_reduce/3","title":"Axon.deep_reduce/3","type":"function"},{"doc":"Adds a dense layer to the network. The dense layer implements: output = activation ( dot ( input , kernel ) + bias ) where activation is given by the :activation option and both kernel and bias are layer parameters. units specifies the number of output units. Compiles to Axon.Layers.dense/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#dense/3","title":"Axon.dense/3","type":"function"},{"doc":"Adds a depthwise convolution layer to the network. The depthwise convolution layer implements a general dimensional depthwise convolution - which is a convolution where the feature group size is equal to the number of input channels. Channel multiplier grows the input channels by the given factor. An input factor of 1 means the output channels are the same as the input channels. Compiles to Axon.Layers.depthwise_conv/4 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#depthwise_conv/3","title":"Axon.depthwise_conv/3","type":"function"},{"doc":"Deserializes serialized model and parameters into a {model, params} tuple. It is the opposite of Axon.serialize/3 . Examples iex&gt; model = Axon . input ( { nil , 2 } , &quot;input&quot; ) |&gt; Axon . dense ( 1 , kernel_initializer : :zeros , activation : :relu ) iex&gt; params = Axon . init ( model ) iex&gt; serialized = Axon . serialize ( model , params ) iex&gt; { saved_model , saved_params } = Axon . deserialize ( serialized ) iex&gt; Axon . predict ( saved_model , saved_params , Nx . tensor ( [ [ 1.0 , 1.0 ] ] ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 0.0 ] ] &gt;","ref":"Axon.html#deserialize/2","title":"Axon.deserialize/2","type":"function"},{"doc":"Adds a Dropout layer to the network. See Axon.Layers.dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 .","ref":"Axon.html#dropout/2","title":"Axon.dropout/2","type":"function"},{"doc":"Adds an Exponential linear unit activation layer to the network. See Axon.Activations.elu/1 for more details. Options :name - layer name.","ref":"Axon.html#elu/2","title":"Axon.elu/2","type":"function"},{"doc":"Adds an embedding layer to the network. An embedding layer initializes a kernel of shape {vocab_size, embedding_size} which acts as a lookup table for sequences of discrete tokens (e.g. sentences). Embeddings are typically used to obtain a dense representation of a sparse input space. Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :uniform .","ref":"Axon.html#embedding/4","title":"Axon.embedding/4","type":"function"},{"doc":"Adds an Exponential activation layer to the network. See Axon.Activations.exp/1 for more details. Options :name - layer name.","ref":"Axon.html#exp/2","title":"Axon.exp/2","type":"function"},{"doc":"Adds a Feature alpha dropout layer to the network. See Axon.Layers.feature_alpha_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 .","ref":"Axon.html#feature_alpha_dropout/2","title":"Axon.feature_alpha_dropout/2","type":"function"},{"doc":"Adds a flatten layer to the network. This layer will flatten all but the batch dimensions of the input into a single layer. Typically called to flatten the output of a convolution for use with a dense layer. Options :name - layer name. :ignore_batch? - whether to ignore batch dimension in transpose operation. Defaults to true .","ref":"Axon.html#flatten/2","title":"Axon.flatten/2","type":"function"},{"doc":"Freezes parameters returned from fun in the given model. fun takes the model's parameter list and returns the list of parameters it wishes to freeze. fun defaults to the identity function, freezing all of the parameters in model . Freezing parameters is useful when performing transfer learning to leverage features learned from another problem in a new problem. For example, it's common to combine the convolutional base from larger models trained on ImageNet with fresh fully-connected classifiers. The combined model is then trained on fresh data, with the convolutional base frozen so as not to lose information. You can see this example in code here: cnn_base = get_pretrained_cnn_base ( ) model = cnn_base |&gt; Axon . freeze ( ) |&gt; Axon . flatten ( ) |&gt; Axon . dense ( 1024 , activation : :relu ) |&gt; Axon . dropout ( ) |&gt; Axon . dense ( 1000 , activation : :softmax ) model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.005 ) ) |&gt; Axon.Loop . run ( data , epochs : 10 ) When compiled, frozen parameters are wrapped in Nx.Defn.Kernel.stop_grad/1 , which zeros out the gradient with respect to the frozen parameter. Gradients of frozen parameters will return 0.0 , meaning they won't be changed during the update process.","ref":"Axon.html#freeze/2","title":"Axon.freeze/2","type":"function"},{"doc":"Adds a Gaussian error linear unit activation layer to the network. See Axon.Activations.gelu/1 for more details. Options :name - layer name.","ref":"Axon.html#gelu/2","title":"Axon.gelu/2","type":"function"},{"doc":"Returns the model's signature as a tuple of {input_shape, output_shape} . Examples iex&gt; model = Axon . input ( { nil , 32 } , &quot;input&quot; ) |&gt; Axon . dense ( 10 ) iex&gt; { inp , out } = Axon . get_model_signature ( model ) iex&gt; inp { nil , 32 } iex&gt; out { nil , 10 } iex&gt; inp1 = Axon . input ( { nil , 32 } , &quot;input_0&quot; ) iex&gt; inp2 = Axon . input ( { nil , 32 } , &quot;input_1&quot; ) iex&gt; model = Axon . concatenate ( inp1 , inp2 ) iex&gt; { { inp1_shape , inp2_shape } , out } = Axon . get_model_signature ( model ) iex&gt; inp1_shape { nil , 32 } iex&gt; inp2_shape { nil , 32 } iex&gt; out { nil , 64 }","ref":"Axon.html#get_model_signature/1","title":"Axon.get_model_signature/1","type":"function"},{"doc":"Adds a Global average pool layer to the network. See Axon.Layers.global_avg_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#global_avg_pool/2","title":"Axon.global_avg_pool/2","type":"function"},{"doc":"Adds a Global LP pool layer to the network. See Axon.Layers.global_lp_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#global_lp_pool/2","title":"Axon.global_lp_pool/2","type":"function"},{"doc":"Adds a Global max pool layer to the network. See Axon.Layers.global_max_pool/2 for more details. Typically used to connect feature extractors such as those in convolutional neural networks to fully-connected models by reducing inputs along spatial dimensions to only feature and batch dimensions. Options :name - layer name. :keep_axes - option to keep reduced axes. If true , keeps reduced axes with a dimension size of 1. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#global_max_pool/2","title":"Axon.global_max_pool/2","type":"function"},{"doc":"Adds a group normalization layer to the network. See Axon.Layers.group_norm/4 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to 1 . :epsilon - numerical stability term.","ref":"Axon.html#group_norm/3","title":"Axon.group_norm/3","type":"function"},{"doc":"See gru/3 .","ref":"Axon.html#gru/2","title":"Axon.gru/2","type":"function"},{"doc":"Adds a gated recurrent unit (GRU) layer to the network with a random initial hidden state. See gru/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :glorot_uniform .","ref":"Axon.html#gru/3","title":"Axon.gru/3","type":"function"},{"doc":"Adds a gated recurrent unit (GRU) layer to the network with the given initial hidden state. GRUs apply Axon.Recurrent.gru_cell/7 over an entire input sequence and return: { { new_hidden } , output_sequence } You can use the output state as the hidden state of another GRU layer. Options :name - layer name. :activation - recurrent activation. Defaults to :tanh . :gate - recurrent gate function. Defaults to :sigmoid . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#gru/4","title":"Axon.gru/4","type":"function"},{"doc":"Adds a Hard sigmoid activation layer to the network. See Axon.Activations.hard_sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_sigmoid/2","title":"Axon.hard_sigmoid/2","type":"function"},{"doc":"Adds a Hard sigmoid weighted linear unit activation layer to the network. See Axon.Activations.hard_silu/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_silu/2","title":"Axon.hard_silu/2","type":"function"},{"doc":"Adds a Hard hyperbolic tangent activation layer to the network. See Axon.Activations.hard_tanh/1 for more details. Options :name - layer name.","ref":"Axon.html#hard_tanh/2","title":"Axon.hard_tanh/2","type":"function"},{"doc":"Compiles and runs the given models initialization function with the given compiler options. You may optionally specify initial parameters for some layers or namespaces by passing a partial parameter map: Axon . init ( model , %{ &quot;dense_0&quot; =&gt; dense_params } ) The parameter map will be merged with the initialized model parameters.","ref":"Axon.html#init/3","title":"Axon.init/3","type":"function"},{"doc":"Adds an input layer to the network. Input layers specify a model's inputs. Input layers are always the root layers of the neural network. You must specify the input layers name, which will be used to uniquely identify it in the case of multiple inputs.","ref":"Axon.html#input/2","title":"Axon.input/2","type":"function"},{"doc":"Adds an Instance normalization layer to the network. See Axon.Layers.instance_norm/6 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to 1 . :epsilon - numerical stability term.","ref":"Axon.html#instance_norm/2","title":"Axon.instance_norm/2","type":"function"},{"doc":"Custom Axon layer with given inputs. Inputs may be other Axon layers or trainable parameters created with Axon.param . At inference time, op will be applied with inputs in specified order and an additional opts parameter which specifies inference options. All options passed to layer are forwarded to inference function except: :shape - specify layer output shape to bypass shape inference. :name - layer name. :op_name - layer operation for inspection and building parameter map. Note this means your layer should not use these as input options, as they will always be dropped during inference compilation. Axon's compiler will additionally forward the following options to every layer at inference time: :mode - :inference or :train . To control layer behavior based on inference or train time. op is a function of the form: fun = fn input , weight , bias , _opts -&gt; input * weight + bias end","ref":"Axon.html#layer/3","title":"Axon.layer/3","type":"function"},{"doc":"Adds a Layer normalization layer to the network. See Axon.Layers.layer_norm/4 for more details. Options :name - layer name. :gamma_initializer - gamma parameter initializer. Defaults to :glorot_uniform . :beta_initializer - beta parameter initializer. Defaults to :zeros . :channel_index - input feature index used for calculating mean and variance. Defaults to 1 . :epsilon - numerical stability term.","ref":"Axon.html#layer_norm/2","title":"Axon.layer_norm/2","type":"function"},{"doc":"Adds a Leaky rectified linear unit activation layer to the network. See Axon.Activations.leaky_relu/1 for more details. Options :name - layer name.","ref":"Axon.html#leaky_relu/2","title":"Axon.leaky_relu/2","type":"function"},{"doc":"Adds a Linear activation layer to the network. See Axon.Activations.linear/1 for more details. Options :name - layer name.","ref":"Axon.html#linear/2","title":"Axon.linear/2","type":"function"},{"doc":"Adds a Log-sigmoid activation layer to the network. See Axon.Activations.log_sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#log_sigmoid/2","title":"Axon.log_sigmoid/2","type":"function"},{"doc":"Adds a Log-softmax activation layer to the network. See Axon.Activations.log_softmax/1 for more details. Options :name - layer name.","ref":"Axon.html#log_softmax/2","title":"Axon.log_softmax/2","type":"function"},{"doc":"Adds a Power average pool layer to the network. See Axon.Layers.lp_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#lp_pool/2","title":"Axon.lp_pool/2","type":"function"},{"doc":"See lstm/3 .","ref":"Axon.html#lstm/2","title":"Axon.lstm/2","type":"function"},{"doc":"Adds a long short-term memory (LSTM) layer to the network with a random initial hidden state. See lstm/4 for more details. Additional options :recurrent_initializer - initializer for hidden state. Defaults to :glorot_uniform .","ref":"Axon.html#lstm/3","title":"Axon.lstm/3","type":"function"},{"doc":"Adds a long short-term memory (LSTM) layer to the network with the given initial hidden state. LSTMs apply Axon.Recurrent.lstm_cell/7 over an entire input sequence and return: { { new_cell , new_hidden } , output_sequence } You can use the output state as the hidden state of another LSTM layer. Options :name - layer name. :activation - recurrent activation. Defaults to :tanh . :gate - recurrent gate function. Defaults to :sigmoid . :unroll - :dynamic (loop preserving) or :static (compiled) unrolling of RNN. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros . :use_bias - whether the layer should add bias to the output. Defaults to true .","ref":"Axon.html#lstm/4","title":"Axon.lstm/4","type":"function"},{"doc":"Adds a Max pool layer to the network. See Axon.Layers.max_pool/2 for more details. Options :name - layer name. :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to size of kernel. :padding - padding to the spatial dimensions of the input. Defaults to :valid . :dilations - window dilations. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#max_pool/2","title":"Axon.max_pool/2","type":"function"},{"doc":"Adds a Mish activation layer to the network. See Axon.Activations.mish/1 for more details. Options :name - layer name.","ref":"Axon.html#mish/2","title":"Axon.mish/2","type":"function"},{"doc":"Adds a multiply layer to the network. This layer performs an element-wise multiply operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#multiply/3","title":"Axon.multiply/3","type":"function"},{"doc":"Wraps an Axon model into a namespace. A namespace is a part of an Axon model which is meant to be a self-contained collection of Axon layers. Namespaces are guaranteed to always generate with the same internal layer names and can be re-used universally across models. Namespaces are most useful for containing large collections of layers and offering a straightforward means for accessing the parameters of individual model components. A common application of namespaces is to use them in with a pre-trained model for fine-tuning: { base , resnet_params } = resnet ( ) base = base |&gt; Axon . namespace ( &quot;resnet&quot; ) model = base |&gt; Axon . dense ( 1 ) Axon . init ( model , %{ &quot;resnset&quot; =&gt; resnet_params } ) Notice you can use Axon.init in conjunction with namespaces to specify which portion of a model you'd like to initialize from a fixed starting point. Namespaces have fixed names, which means it's easy to run into namespace collisions. Re-using namespaces, re-using inner parts of a namespace, and attempting to share layers between namespaces are still sharp edges in namespace usage.","ref":"Axon.html#namespace/2","title":"Axon.namespace/2","type":"function"},{"doc":"Applies the given Nx expression to the input. Nx layers are meant for quick applications of functions without trainable parameters. For example, they are useful for applying functions which apply accessors to containers: model = Axon . container ( { foo , bar } ) Axon . nx ( model , &amp; elem ( &amp;1 , 0 ) ) Options :name - layer name.","ref":"Axon.html#nx/3","title":"Axon.nx/3","type":"function"},{"doc":"Adds a pad layer to the network. This layer will pad the spatial dimensions of the input. Padding configuration is a list of tuples for each spatial dimension. Options :name - layer name. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#pad/4","title":"Axon.pad/4","type":"function"},{"doc":"Trainable Axon parameter used to create custom layers. Parameters are specified in usages of Axon.layer and will be automatically initialized and used in subsequent applications of Axon models. Parameters must be specified in order of their usage. Options :initializer - parameter initializer. Defaults to :glorot_uniform .","ref":"Axon.html#param/3","title":"Axon.param/3","type":"function"},{"doc":"Compiles and runs the given Axon model with params on input with the given compiler options.","ref":"Axon.html#predict/4","title":"Axon.predict/4","type":"function"},{"doc":"Adds a Rectified linear unit 6 activation layer to the network. See Axon.Activations.relu6/1 for more details. Options :name - layer name.","ref":"Axon.html#relu6/2","title":"Axon.relu6/2","type":"function"},{"doc":"Adds a Rectified linear unit activation layer to the network. See Axon.Activations.relu/1 for more details. Options :name - layer name.","ref":"Axon.html#relu/2","title":"Axon.relu/2","type":"function"},{"doc":"Adds a reshape layer to the network. This layer implements a special case of Nx.reshape which accounts for possible batch dimensions in the input tensor. If the input contains batch dimensions, the reshape operation is performed on all non-batch dimensions of the input - preserving the original batch size. If the input is an Axon constant, the reshape behavior matches that of Nx.reshape . Options :name - layer name. :ignore_batch? - whether to ignore batch dimension in transpose operation. Defaults to true .","ref":"Axon.html#reshape/3","title":"Axon.reshape/3","type":"function"},{"doc":"Adds a resize layer to the network. Resizing can be used for interpolation or upsampling input values in a neural network. For example, you can use this layer as an upsampling layer within a GAN. Resize shape must be a tuple representing the resized spatial dimensions of the input tensor. Compiles to Axon.Layers.resize/2 . Options :name - layer name. :method - resize method. Defaults to :nearest . :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.html#resize/3","title":"Axon.resize/3","type":"function"},{"doc":"Adds a Scaled exponential linear unit activation layer to the network. See Axon.Activations.selu/1 for more details. Options :name - layer name.","ref":"Axon.html#selu/2","title":"Axon.selu/2","type":"function"},{"doc":"Adds a depthwise separable 2-dimensional convolution to the network. Depthwise separable convolutions break the kernel into kernels for each dimension of the input and perform a depthwise conv over the input with each kernel. Compiles to Axon.Layers.separable_conv2d/6 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#separable_conv2d/3","title":"Axon.separable_conv2d/3","type":"function"},{"doc":"Adds a depthwise separable 3-dimensional convolution to the network. Depthwise separable convolutions break the kernel into kernels for each dimension of the input and perform a depthwise conv over the input with each kernel. Compiles to Axon.Layers.separable_conv3d/8 . Options :name - layer name. :kernel_initializer - initializer for kernel weights. Defaults to :glorot_uniform . :bias_initializer - initializer for bias weights. Defaults to :zeros :activation - element-wise activation function. :use_bias - whether the layer should add bias to the output. Defaults to true :kernel_size - size of the kernel spatial dimensions. Defaults to 1 . :strides - stride during convolution. Defaults to 1 . :padding - padding to the spatial dimensions of the input. Defaults to :valid . :input_dilation - dilation to apply to input. Defaults to 1 . :kernel_dilation - dilation to apply to kernel. Defaults to 1 . :channels - channels location. One of :first or :last . Defaults to :first .","ref":"Axon.html#separable_conv3d/3","title":"Axon.separable_conv3d/3","type":"function"},{"doc":"Serializes a model and its parameters for persisting models to disk or elsewhere. Model and parameters are serialized as a tuple, where the model is converted to a recursive map to ensure compatibility with future Axon versions and the parameters are serialized using Nx.serialize/2 . There is some additional metadata included such as current serialization version for compatibility. Serialization opts are forwarded to Nx.serialize/2 and :erlang.term_to_binary/2 for controlling compression options. Examples iex&gt; model = Axon . input ( { nil , 2 } , &quot;input&quot; ) |&gt; Axon . dense ( 1 , kernel_initializer : :zeros , activation : :relu ) iex&gt; params = Axon . init ( model ) iex&gt; serialized = Axon . serialize ( model , params ) iex&gt; { saved_model , saved_params } = Axon . deserialize ( serialized ) iex&gt; Axon . predict ( saved_model , saved_params , Nx . tensor ( [ [ 1.0 , 1.0 ] ] ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ [ 0.0 ] ] &gt;","ref":"Axon.html#serialize/3","title":"Axon.serialize/3","type":"function"},{"doc":"Adds a Sigmoid activation layer to the network. See Axon.Activations.sigmoid/1 for more details. Options :name - layer name.","ref":"Axon.html#sigmoid/2","title":"Axon.sigmoid/2","type":"function"},{"doc":"Adds a Sigmoid weighted linear unit activation layer to the network. See Axon.Activations.silu/1 for more details. Options :name - layer name.","ref":"Axon.html#silu/2","title":"Axon.silu/2","type":"function"},{"doc":"Adds a Softmax activation layer to the network. See Axon.Activations.softmax/1 for more details. Options :name - layer name.","ref":"Axon.html#softmax/2","title":"Axon.softmax/2","type":"function"},{"doc":"Adds a Softplus activation layer to the network. See Axon.Activations.softplus/1 for more details. Options :name - layer name.","ref":"Axon.html#softplus/2","title":"Axon.softplus/2","type":"function"},{"doc":"Adds a Softsign activation layer to the network. See Axon.Activations.softsign/1 for more details. Options :name - layer name.","ref":"Axon.html#softsign/2","title":"Axon.softsign/2","type":"function"},{"doc":"Adds a Spatial dropout layer to the network. See Axon.Layers.spatial_dropout/2 for more details. Options :name - layer name. :rate - dropout rate. Defaults to 0.5 .","ref":"Axon.html#spatial_dropout/2","title":"Axon.spatial_dropout/2","type":"function"},{"doc":"Splits input graph into a container of n input graphs along the given axis. Options :name - layer name. :axis - concatenate axis. Defaults to -1 .","ref":"Axon.html#split/3","title":"Axon.split/3","type":"function"},{"doc":"Adds a subtract layer to the network. This layer performs an element-wise subtract operation on input layers. All input layers must be capable of being broadcast together. If one shape has a static batch size, all other shapes must have a static batch size as well. Options :name - layer name.","ref":"Axon.html#subtract/3","title":"Axon.subtract/3","type":"function"},{"doc":"Adds a Hyperbolic tangent activation layer to the network. See Axon.Activations.tanh/1 for more details. Options :name - layer name.","ref":"Axon.html#tanh/2","title":"Axon.tanh/2","type":"function"},{"doc":"Adds a transpose layer to the network. Options :name - layer name. :ignore_batch? - whether to ignore batch dimension in transpose operation. Defaults to true.","ref":"Axon.html#transpose/3","title":"Axon.transpose/3","type":"function"},{"doc":"Traverses a model tree applying fun to each layer.","ref":"Axon.html#tree_map/2","title":"Axon.tree_map/2","type":"function"},{"doc":"Traverses a model applying fun with an accumulator.","ref":"Axon.html#tree_reduce/3","title":"Axon.tree_reduce/3","type":"function"},{"doc":"","ref":"Axon.html#t:t/0","title":"Axon.t/0","type":"type"},{"doc":"Parameter initializers. Parameter initializers are used to initialize the weights and biases of a neural network. Because most deep learning optimization algorithms are iterative, they require an initial point to iterate from. Sometimes the initialization of a model can determine whether or not a model converges. In some cases, the initial point is unstable, and therefore the model has no chance of converging using common first-order optimization methods. In cases where the model will converge, initialization can have a significant impact on how quickly the model converges. Most initialization strategies are built from intuition and heuristics rather than theory. It's commonly accepted that the parameters of different layers should be different - motivating the use of random initialization for each layer's parameters. Usually, only the weights of a layer are initialized using a random distribution - while the biases are initialized to a uniform constant (like 0). Most initializers use Gaussian (normal) or uniform distributions with variations on scale. The output scale of an initializer should generally be large enough to avoid information loss but small enough to avoid exploding values. The initializers in this module have a default scale known to work well with the initialization strategy. The functions in this module return initialization functions which take shapes and types and return tensors: init_fn = Axon.Initializers . zeros ( ) init_fn . ( { 1 , 2 } , { :f , 32 } ) You may use these functions from within defn or outside.","ref":"Axon.Initializers.html","title":"Axon.Initializers","type":"module"},{"doc":"Initializes parameters to value. Examples iex&gt; init_fn = Axon.Initializers . full ( 1.00 ) iex&gt; init_fn . ( { 2 , 2 } , { :f , 32 } ) # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 1.0 ] , [ 1.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#full/1","title":"Axon.Initializers.full/1","type":"function"},{"doc":"Initializes parameters with the Glorot normal initializer. The Glorot normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_avg and distribution: :truncated_normal . The Glorot normal initializer is also called the Xavier normal initializer. Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . glorot_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . glorot_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Understanding the difficulty of training deep feedforward neural networks","ref":"Axon.Initializers.html#glorot_normal/1","title":"Axon.Initializers.glorot_normal/1","type":"function"},{"doc":"Initializes parameters with the Glorot uniform initializer. The Glorot uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_avg and distribution: :uniform . The Glorot uniform initializer is also called the Xavier uniform initializer. Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . glorot_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . glorot_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Understanding the difficulty of training deep feedforward neural networks","ref":"Axon.Initializers.html#glorot_uniform/1","title":"Axon.Initializers.glorot_uniform/1","type":"function"},{"doc":"Initializes parameters with the He normal initializer. The He normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :truncated_normal . Options :scale - scale of the output distribution. Defaults to 2.0 Examples iex&gt; init_fn = Axon.Initializers . he_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . he_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","ref":"Axon.Initializers.html#he_normal/1","title":"Axon.Initializers.he_normal/1","type":"function"},{"doc":"Initializes parameters with the He uniform initializer. The He uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_ni and distribution: :uniform . Options :scale - scale of the output distribution. Defaults to 2.0 Examples iex&gt; init_fn = Axon.Initializers . he_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . he_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification","ref":"Axon.Initializers.html#he_uniform/1","title":"Axon.Initializers.he_uniform/1","type":"function"},{"doc":"Initializes parameters to an identity matrix. Examples iex&gt; init_fn = Axon.Initializers . identity ( ) iex&gt; init_fn . ( { 2 , 2 } , { :f , 32 } ) # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 0.0 ] , [ 0.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#identity/0","title":"Axon.Initializers.identity/0","type":"function"},{"doc":"Initializes parameters with the Lecun normal initializer. The Lecun normal initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :truncated_normal . Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . lecun_normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . lecun_normal ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Efficient BackProp","ref":"Axon.Initializers.html#lecun_normal/1","title":"Axon.Initializers.lecun_normal/1","type":"function"},{"doc":"Initializes parameters with the Lecun uniform initializer. The Lecun uniform initializer is equivalent to calling Axon.Initializers.variance_scaling with mode: :fan_in and distribution: :uniform . Options :scale - scale of the output distribution. Defaults to 1.0 Examples iex&gt; init_fn = Axon.Initializers . lecun_uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . lecun_uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } References Efficient BackProp","ref":"Axon.Initializers.html#lecun_uniform/1","title":"Axon.Initializers.lecun_uniform/1","type":"function"},{"doc":"Initializes parameters with a random normal distribution. Options :mean - mean of the output distribution. Defaults to 0.0 :scale - scale of the output distribution. Defaults to 1.0e-2 Examples iex&gt; init_fn = Axon.Initializers . normal ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . normal ( mean : 1.0 , scale : 1.0 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 }","ref":"Axon.Initializers.html#normal/1","title":"Axon.Initializers.normal/1","type":"function"},{"doc":"Initializes parameters to 1. Examples iex&gt; init_fn = Axon.Initializers . ones ( ) iex&gt; init_fn . ( { 2 , 2 } , { :f , 32 } ) # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 1.0 , 1.0 ] , [ 1.0 , 1.0 ] ] &gt;","ref":"Axon.Initializers.html#ones/0","title":"Axon.Initializers.ones/0","type":"function"},{"doc":"Initializes a tensor with an orthogonal distribution. For 2-D tensors, the initialization is generated through the QR decomposition of a random distribution For tensors with more than 2 dimensions, a 2-D tensor with shape {shape[0] * shape[1] * ... * shape[n-2], shape[n-1]} is initialized and then reshaped accordingly. Options :distribution - output distribution. One of [ :normal , :uniform ]. Defaults to :normal Examples iex&gt; init_fn = Axon.Initializers . orthogonal ( ) iex&gt; t = init_fn . ( { 3 , 3 } , { :f , 32 } ) iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; Nx . shape ( t ) { 3 , 3 } iex&gt; init_fn = Axon.Initializers . orthogonal ( ) iex&gt; t = init_fn . ( { 1 , 2 , 3 , 4 } , { :f , 64 } ) iex&gt; Nx . type ( t ) { :f , 64 } iex&gt; Nx . shape ( t ) { 1 , 2 , 3 , 4 }","ref":"Axon.Initializers.html#orthogonal/1","title":"Axon.Initializers.orthogonal/1","type":"function"},{"doc":"Initializes parameters with a random uniform distribution. Options :scale - scale of the output distribution. Defaults to 1.0e-2 Examples iex&gt; init_fn = Axon.Initializers . uniform ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . uniform ( scale : 1.0e-3 ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 }","ref":"Axon.Initializers.html#uniform/1","title":"Axon.Initializers.uniform/1","type":"function"},{"doc":"Initializes parameters with variance scaling according to the given distribution and mode. Variance scaling adapts scale to the weights of the output tensor. Options :scale - scale of the output distribution. Defaults to 1.0e-2 :mode - compute fan mode. One of :fan_in , :fan_out , or :fan_avg . Defaults to :fan_in :distribution - output distribution. One of :normal , :truncated_normal , or :uniform . Defaults to :normal Examples iex&gt; init_fn = Axon.Initializers . variance_scaling ( ) iex&gt; t = init_fn . ( { 2 , 2 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :f , 32 } iex&gt; init_fn = Axon.Initializers . variance_scaling ( mode : :fan_out , distribution : :truncated_normal ) iex&gt; t = init_fn . ( { 2 , 2 } , { :bf , 16 } ) iex&gt; Nx . shape ( t ) { 2 , 2 } iex&gt; Nx . type ( t ) { :bf , 16 } iex&gt; init_fn = Axon.Initializers . variance_scaling ( mode : :fan_out , distribution : :normal ) iex&gt; t = init_fn . ( { 64 , 3 , 32 , 32 } , { :f , 32 } ) iex&gt; Nx . shape ( t ) { 64 , 3 , 32 , 32 } iex&gt; Nx . type ( t ) { :f , 32 }","ref":"Axon.Initializers.html#variance_scaling/1","title":"Axon.Initializers.variance_scaling/1","type":"function"},{"doc":"Initializes parameters to 0. Examples iex&gt; init_fn = Axon.Initializers . zeros ( ) iex&gt; init_fn . ( { 2 , 2 } , { :f , 32 } ) # Nx.Tensor &lt; f32 [ 2 ] [ 2 ] [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] &gt;","ref":"Axon.Initializers.html#zeros/0","title":"Axon.Initializers.zeros/0","type":"function"},{"doc":"Utilities for creating mixed precision policies. Mixed precision is useful for increasing model throughput at the possible price of a small dip in accuracy. When creating a mixed precision policy, you define the policy for params , compute , and output . The params policy dictates what type parameters should be stored as during training. The compute policy dictates what type should be used during intermediate computations in the model's forward pass. The output policy dictates what type the model should output. Here's an example of creating a mixed precision policy and applying it to a model: model = Axon . input ( { nil , 784 } ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 64 , activation : :relu ) |&gt; Axon . batch_norm ( ) |&gt; Axon . dropout ( rate : 0.5 ) |&gt; Axon . dense ( 10 , activation : :softmax ) policy = Axon.MixedPrecision . create_policy ( params : { :f , 32 } , compute : { :f , 16 } , output : { :f , 32 } ) mp_model = model |&gt; Axon.MixedPrecision . apply_policy ( policy , except : [ :batch_norm ] ) The example above applies the mixed precision policy to every layer in the model except Batch Normalization layers. The policy will cast parameters and inputs to {:f, 16} for intermediate computations in the model's forward pass before casting the output back to {:f, 32} .","ref":"Axon.MixedPrecision.html","title":"Axon.MixedPrecision","type":"module"},{"doc":"Creates a mixed precision policy with the given options. Options params - parameter precision policy. Defaults to {:f, 32} compute - compute precision policy. Defaults to {:f, 32} output - output precision policy. Defaults to {:f, 32} Examples iex&gt; Axon.MixedPrecision . create_policy ( params : { :f , 16 } , output : { :f , 16 } ) % Policy { params : { :f , 16 } , compute : { :f , 32 } , output : { :f , 16 } } iex&gt; Axon.MixedPrecision . create_policy ( compute : { :bf , 16 } ) % Policy { params : { :f , 32 } , compute : { :bf , 16 } , output : { :f , 32 } }","ref":"Axon.MixedPrecision.html#create_policy/1","title":"Axon.MixedPrecision.create_policy/1","type":"function"},{"doc":"Parameter Schedules. Parameter schedules are often used to anneal hyperparameters such as the learning rate during the training process. Schedules provide a mapping from the current time step to a learning rate or another hyperparameter. Choosing a good learning rate and consequently a good learning rate schedule is typically a process of trial and error. Learning rates should be relatively small such that the learning curve does not oscillate violently during the training process, but not so small that learning proceeds too slowly. Using a schedule slowly decreases oscillations during the training process such that, as the model converges, training also becomes more stable. All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Schedules.html","title":"Axon.Schedules","type":"module"},{"doc":"Constant schedule. $$\\gamma(t) = \\gamma_0$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2","ref":"Axon.Schedules.html#constant/1","title":"Axon.Schedules.constant/1","type":"function"},{"doc":"Cosine decay schedule. $$\\gamma(t) = \\gamma_0 (1 - \\alpha) (\\frac{1}{2}(1 + \\cos{\\pi \\frac{t}{k}})) + \\alpha$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2 :decay_steps - number of steps to apply decay for. $k$ in above formulation. Defaults to 10 :alpha - minimum value of multiplier adjusting learning rate. $\\alpha$ in above formulation. Defaults to 0.0 References SGDR: Stochastic Gradient Descent with Warm Restarts","ref":"Axon.Schedules.html#cosine_decay/1","title":"Axon.Schedules.cosine_decay/1","type":"function"},{"doc":"Exponential decay schedule. $$\\gamma(t) = \\gamma_0 * r^{\\frac{t}{k}}$$ Options :init_value - initial value. $\\gamma$ in above formulation. Defaults to 1.0e-2 :decay_rate - rate of decay. $r$ in above formulation. Defaults to 0.95 :transition_steps - steps per transition. $k$ in above formulation. Defaults to 10 :transition_begin - step to begin transition. Defaults to 0 :staircase - discretize outputs. Defaults to false","ref":"Axon.Schedules.html#exponential_decay/1","title":"Axon.Schedules.exponential_decay/1","type":"function"},{"doc":"Polynomial schedule. $$\\gamma(t) = (\\gamma_0 - \\gamma_n) * (1 - \\frac{t}{k})^p$$ Options :init_value - initial value. $\\gamma_0$ in above formulation. Defaults to 1.0e-2 :end_value - end value of annealed scalar. $\\gamma_n$ in above formulation. Defaults to 1.0e-3 :power - power of polynomial. $p$ in above formulation. Defaults to 2 :transition_steps - number of steps over which annealing takes place. $k$ in above formulation. Defaults to 10","ref":"Axon.Schedules.html#polynomial_decay/1","title":"Axon.Schedules.polynomial_decay/1","type":"function"},{"doc":"Container for returning stateful outputs from Axon layers. Some layers, such as Axon.batch_norm/2 , keep a running internal state which is updated continuously at train time and used statically at inference time. In order for the Axon compiler to differentiate ordinary layer outputs from internal state, you must mark output as stateful. Stateful Outputs consist of two fields: :output - Actual layer output to be forwarded to next layer :state - Internal layer state to be tracked and updated :output is simply forwarded to the next layer. :state is aggregated with other stateful outputs, and then is treated specially by internal Axon training functions such that update state parameters reflect returned values from stateful outputs. :state must be a map with keys that map directly to layer internal state names. For example, Axon.Layers.batch_norm returns StatefulOutput with :state keys of &quot;mean&quot; and &quot;var&quot; .","ref":"Axon.StatefulOutput.html","title":"Axon.StatefulOutput","type":"module"},{"doc":"Activation functions. Activation functions are element-wise, (typically) non-linear functions called on the output of another layer, such as a dense layer: x |&gt; dense ( weight , bias ) |&gt; relu ( ) Activation functions output the &quot;activation&quot; or how active a given layer's neurons are in learning a representation of the data-generating distribution. Some activations are commonly used as output activations. For example softmax is often used as the output in multiclass classification problems because it returns a categorical probability distribution: iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ 1 , 2 , 3 ] ] , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ [ 0.09003057330846786 , 0.2447284758090973 , 0.6652409434318542 ] ] &gt; Other activations such as tanh or sigmoid are used because they have desirable properties, such as keeping the output tensor constrained within a certain range. Generally, the choice of activation function is arbitrary; although some activations work better than others in certain problem domains. For example ReLU (rectified linear unit) activation is a widely-accepted default. You can see a list of activation functions and implementations here . All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Activations.html","title":"Axon.Activations","type":"module"},{"doc":"Continuously-differentiable exponential linear unit activation. $$f(x_i) = \\max(0, x_i) + \\min(0, \\alpha * e^{\\frac{x_i}{\\alpha}} - 1)$$ Options alpha - $\\alpha$ in CELU formulation. Must be non-zero. Defaults to 1.0 Examples iex&gt; Axon.Activations . celu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ - 0.9502129554748535 , - 0.8646647334098816 , - 0.6321205496788025 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . celu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } ) ) # Nx.Tensor &lt; bf16 [ 2 ] [ 3 ] [ [ - 0.62890625 , - 0.86328125 , - 0.94921875 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; Error cases iex&gt; Axon.Activations . celu ( Nx . tensor ( [ 0.0 , 1.0 , 2.0 ] , type : { :f , 32 } ) , alpha : 0.0 ) ** (ArgumentError) :alpha must be non-zero in CELU activation References Continuously Differentiable Exponential Linear Units","ref":"Axon.Activations.html#celu/2","title":"Axon.Activations.celu/2","type":"function"},{"doc":"Exponential linear unit activation. Equivalent to celu for $\\alpha = 1$ $$f(x_i) = \\begin{cases}x_i &amp; x _i &gt; 0 \\newline \\alpha * (e^{x_i} - 1) &amp; x_i \\leq 0 \\ \\end{cases}$$ Options alpha - $\\alpha$ in ELU formulation. Defaults to 1.0 Examples iex&gt; Axon.Activations . elu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ - 0.9502129554748535 , - 0.8646647334098816 , - 0.6321205496788025 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . elu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } ) ) # Nx.Tensor &lt; bf16 [ 2 ] [ 3 ] [ [ - 0.62890625 , - 0.86328125 , - 0.94921875 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; References Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","ref":"Axon.Activations.html#elu/2","title":"Axon.Activations.elu/2","type":"function"},{"doc":"Exponential activation. $$f(x_i) = e^{x_i}$$ Examples iex&gt; Axon.Activations . exp ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.049787066876888275 , 0.1353352814912796 , 0.3678794503211975 , 1.0 , 2.7182817459106445 , 7.389056205749512 , 20.08553695678711 ] &gt; iex&gt; Axon.Activations . exp ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.3671875 , 0.134765625 , 0.049560546875 ] , [ 2.703125 , 7.375 , 20.0 ] ] &gt;","ref":"Axon.Activations.html#exp/1","title":"Axon.Activations.exp/1","type":"function"},{"doc":"Gaussian error linear unit activation. $$f(x_i) = \\frac{x_i}{2}(1 + {erf}(\\frac{x_i}{\\sqrt{2}}))$$ Examples iex&gt; Axon.Activations . gelu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.0040496885776519775 , - 0.04550027847290039 , - 0.15865525603294373 , 0.0 , 0.8413447141647339 , 1.9544997215270996 , 2.995950222015381 ] &gt; iex&gt; Axon.Activations . gelu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.16015625 , - 0.046875 , - 0.005859375 ] , [ 0.83984375 , 1.953125 , 2.984375 ] ] &gt; References Gaussian Error Linear Units (GELUs)","ref":"Axon.Activations.html#gelu/1","title":"Axon.Activations.gelu/1","type":"function"},{"doc":"Hard sigmoid activation. Examples iex&gt; Axon.Activations . hard_sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.0 , 0.0 , 0.0 , 0.20000000298023224 , 0.4000000059604645 , 0.6000000238418579 , 0.800000011920929 ] &gt; iex&gt; Axon.Activations . hard_sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 7.781982421875e-4 , 0.0 , 0.0 ] , [ 0.3984375 , 0.59765625 , 0.796875 ] ] &gt;","ref":"Axon.Activations.html#hard_sigmoid/2","title":"Axon.Activations.hard_sigmoid/2","type":"function"},{"doc":"Hard sigmoid weighted linear unit activation. $$f(x_i) = \\begin{cases} 0 &amp; x_i \\leq -3 \\newline x &amp; x_i \\geq 3 \\newline \\frac{x_i^2}{6} + \\frac{x_i}{2} &amp; otherwise \\end{cases}$$ Examples iex&gt; Axon.Activations . hard_silu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.0 , - 0.0 , - 0.0 , 0.0 , 0.4000000059604645 , 1.2000000476837158 , 2.4000000953674316 ] &gt; iex&gt; Axon.Activations . hard_silu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 7.781982421875e-4 , - 0.0 , - 0.0 ] , [ 0.3984375 , 1.1953125 , 2.390625 ] ] &gt;","ref":"Axon.Activations.html#hard_silu/2","title":"Axon.Activations.hard_silu/2","type":"function"},{"doc":"Hard hyperbolic tangent activation. $$f(x_i) = \\begin{cases} 1 &amp; x &gt; 1 \\newline -1 &amp; x &lt; -1 \\newline x &amp; otherwise \\end{cases}$$ Examples iex&gt; Axon.Activations . hard_tanh ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.0 , - 1.0 , - 1.0 , 0.0 , 1.0 , 1.0 , 1.0 ] &gt; iex&gt; Axon.Activations . hard_tanh ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.0 , - 1.0 , - 1.0 ] , [ 1.0 , 1.0 , 1.0 ] ] &gt;","ref":"Axon.Activations.html#hard_tanh/1","title":"Axon.Activations.hard_tanh/1","type":"function"},{"doc":"Leaky rectified linear unit activation. $$f(x_i) = \\begin{cases} x &amp; x \\geq 0 \\newline \\alpha * x &amp; otherwise \\end{cases}$$ Options :alpha - $\\alpha$ in Leaky ReLU formulation. Defaults to 1.0e-2 Examples iex&gt; Axon.Activations . leaky_relu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) , alpha : 0.5 ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.5 , - 1.0 , - 0.5 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . leaky_relu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , names : [ :batch , :data ] ) , alpha : 0.5 ) # Nx.Tensor &lt; f32 [ batch : 2 ] [ data : 3 ] [ [ - 0.5 , - 1.0 , - 1.5 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#leaky_relu/2","title":"Axon.Activations.leaky_relu/2","type":"function"},{"doc":"Linear activation. $$f(x_i) = x_i$$ Examples iex&gt; Axon.Activations . linear ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . linear ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#linear/1","title":"Axon.Activations.linear/1","type":"function"},{"doc":"Log-sigmoid activation. $$f(x_i) = \\log(\\sigmoid(x))$$ Examples iex&gt; Axon.Activations . log_sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 3.0485873222351074 , - 2.1269280910491943 , - 1.3132617473602295 , - 0.6931471824645996 , - 0.3132616877555847 , - 0.12692801654338837 , - 0.04858734831213951 ] &gt; iex&gt; Axon.Activations . log_sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.3125 , - 2.125 , - 3.046875 ] , [ - 0.3125 , - 0.1259765625 , - 0.04833984375 ] ] &gt;","ref":"Axon.Activations.html#log_sigmoid/1","title":"Axon.Activations.log_sigmoid/1","type":"function"},{"doc":"Log-softmax activation. $$f(x_i) = -log( um{e^x_i})$$ Examples iex&gt; Axon.Activations . log_softmax ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 6.457762718200684 , - 5.457762718200684 , - 4.457762718200684 , - 3.4577627182006836 , - 2.4577627182006836 , - 1.4577628374099731 , - 0.45776283740997314 ] &gt; iex&gt; Axon.Activations . log_softmax ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.404296875 , - 1.3984375 , - 2.390625 ] , [ - 2.390625 , - 1.3984375 , - 0.404296875 ] ] &gt;","ref":"Axon.Activations.html#log_softmax/2","title":"Axon.Activations.log_softmax/2","type":"function"},{"doc":"Mish activation. $$f(x_i) = x_i* \\tanh(\\log(1 + e^x_i))$$ Examples iex&gt; Axon.Activations . mish ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , type : { :f , 32 } , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.14564745128154755 , - 0.2525014877319336 , - 0.30340147018432617 , 0.0 , 0.8650984168052673 , 1.9439589977264404 , 2.98653507232666 ] &gt; iex&gt; Axon.Activations . mish ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.30078125 , - 0.25 , - 0.1435546875 ] , [ 0.86328125 , 1.9375 , 2.96875 ] ] &gt;","ref":"Axon.Activations.html#mish/1","title":"Axon.Activations.mish/1","type":"function"},{"doc":"Rectified linear unit 6 activation. $$f(x_i) = \\min_i(\\max_i(x, 0), 6)$$ Examples iex&gt; Axon.Activations . relu6 ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ) ) # Nx.Tensor &lt; f32 [ 7 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . relu6 ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.0 , 0.0 , 0.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt; References MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","ref":"Axon.Activations.html#relu6/1","title":"Axon.Activations.relu6/1","type":"function"},{"doc":"Rectified linear unit activation. $$f(x_i) = \\max_i(x, 0)$$ Examples iex&gt; Axon.Activations . relu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 2.0 , 3.0 ] &gt; iex&gt; Axon.Activations . relu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.0 , 0.0 , 0.0 ] , [ 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Activations.html#relu/1","title":"Axon.Activations.relu/1","type":"function"},{"doc":"Scaled exponential linear unit activation. $$f(x_i) = \\begin{cases} \\lambda x &amp; x \\geq 0 \\newline \\lambda \\alpha(e^{x} - 1) &amp; x &lt; 0 \\end{cases}$$ $$\\alpha \\approx 1.6733$$ $$\\lambda \\approx 1.0507$$ Examples iex&gt; Axon.Activations . selu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 1.670568823814392 , - 1.5201665163040161 , - 1.1113307476043701 , 0.0 , 1.0507010221481323 , 2.1014020442962646 , 3.1521029472351074 ] &gt; iex&gt; Axon.Activations . selu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 1.09375 , - 1.5078125 , - 1.6640625 ] , [ 1.046875 , 2.09375 , 3.140625 ] ] &gt; References Self-Normalizing Neural Networks","ref":"Axon.Activations.html#selu/2","title":"Axon.Activations.selu/2","type":"function"},{"doc":"Sigmoid activation. $$f(x_i) = \\frac{1}{1 + e^{-x_i}}$$ Implementation Note: Sigmoid logits are cached as metadata in the expression and can be used in calculations later on. For example, they are used in cross-entropy calculations for better stability. Examples iex&gt; Axon.Activations . sigmoid ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.04742587357759476 , 0.11920291930437088 , 0.2689414322376251 , 0.5 , 0.7310585975646973 , 0.8807970881462097 , 0.9525741338729858 ] &gt; iex&gt; Axon.Activations . sigmoid ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.267578125 , 0.119140625 , 0.04736328125 ] , [ 0.73046875 , 0.87890625 , 0.94921875 ] ] &gt;","ref":"Axon.Activations.html#sigmoid/1","title":"Axon.Activations.sigmoid/1","type":"function"},{"doc":"Sigmoid weighted linear unit activation. $$f(x_i) = x\\sigmoid(x)$$ Examples iex&gt; Axon.Activations . silu ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.14227762818336487 , - 0.23840583860874176 , - 0.2689414322376251 , 0.0 , 0.7310585975646973 , 1.7615941762924194 , 2.857722282409668 ] &gt; iex&gt; Axon.Activations . silu ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.267578125 , - 0.23828125 , - 0.1416015625 ] , [ 0.73046875 , 1.7578125 , 2.84375 ] ] &gt; References Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning","ref":"Axon.Activations.html#silu/1","title":"Axon.Activations.silu/1","type":"function"},{"doc":"Softmax activation along an axis. $$\\frac{e^{x_i}}{\\sum_i e^{x_i}}$$ Implementation Note: Sigmoid logits are cached as metadata in the expression and can be used in calculations later on. For example, they are used in cross-entropy calculations for better stability. Options :axis - softmax axis along which to calculate distribution. Defaults to 1. Examples iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] ] , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; f32 [ batch : 1 ] [ data : 7 ] [ [ 0.0015683004166930914 , 0.004263082519173622 , 0.011588259600102901 , 0.03150015324354172 , 0.08562629669904709 , 0.23275642096996307 , 0.6326975226402283 ] ] &gt; iex&gt; Axon.Activations . softmax ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.6640625 , 0.2431640625 , 0.08935546875 ] , [ 0.08935546875 , 0.2431640625 , 0.6640625 ] ] &gt;","ref":"Axon.Activations.html#softmax/2","title":"Axon.Activations.softmax/2","type":"function"},{"doc":"Softplus activation. $$\\log(1 + e^x_i)$$ Examples iex&gt; Axon.Activations . softplus ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ 0.04858734831213951 , 0.12692801654338837 , 0.3132616877555847 , 0.6931471824645996 , 1.3132617473602295 , 2.1269280910491943 , 3.0485873222351074 ] &gt; iex&gt; Axon.Activations . softplus ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ 0.3125 , 0.1259765625 , 0.04833984375 ] , [ 1.3125 , 2.125 , 3.046875 ] ] &gt;","ref":"Axon.Activations.html#softplus/1","title":"Axon.Activations.softplus/1","type":"function"},{"doc":"Softsign activation. $$f(x_i) = \\frac{x_i}{|x_i| + 1}$$ Examples iex&gt; Axon.Activations . softsign ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.75 , - 0.6666666865348816 , - 0.5 , 0.0 , 0.5 , 0.6666666865348816 , 0.75 ] &gt; iex&gt; Axon.Activations . softsign ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.5 , - 0.6640625 , - 0.75 ] , [ 0.5 , 0.6640625 , 0.75 ] ] &gt;","ref":"Axon.Activations.html#softsign/1","title":"Axon.Activations.softsign/1","type":"function"},{"doc":"Hyperbolic tangent activation. $$f(x_i) = \\tanh(x_i)$$ Examples iex&gt; Axon.Activations . tanh ( Nx . tensor ( [ - 3.0 , - 2.0 , - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 ] , names : [ :data ] ) ) # Nx.Tensor &lt; f32 [ data : 7 ] [ - 0.9950547814369202 , - 0.9640275835990906 , - 0.7615941762924194 , 0.0 , 0.7615941762924194 , 0.9640275835990906 , 0.9950547814369202 ] &gt; iex&gt; Axon.Activations . tanh ( Nx . tensor ( [ [ - 1.0 , - 2.0 , - 3.0 ] , [ 1.0 , 2.0 , 3.0 ] ] , type : { :bf , 16 } , names : [ :batch , :data ] ) ) # Nx.Tensor &lt; bf16 [ batch : 2 ] [ data : 3 ] [ [ - 0.7578125 , - 0.9609375 , - 0.9921875 ] , [ 0.7578125 , 0.9609375 , 0.9921875 ] ] &gt;","ref":"Axon.Activations.html#tanh/1","title":"Axon.Activations.tanh/1","type":"function"},{"doc":"Functional implementations of common neural network layer operations. Layers are the building blocks of neural networks. These functional implementations can be used to express higher-level constructs using fundamental building blocks. Neural network layers are stateful with respect to their parameters. These implementations do not assume the responsibility of managing state - instead opting to delegate this responsibility to the caller. Basic neural networks can be seen as a composition of functions: input |&gt; dense ( w1 , b1 ) |&gt; relu ( ) |&gt; dense ( w2 , b2 ) |&gt; softmax ( ) These kinds of models are often referred to as deep feedforward networks or multilayer perceptrons (MLPs) because information flows forward through the network with no feedback connections. Mathematically, a feedforward network can be represented as: $$f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$$ You can see a similar pattern emerge if we condense the call stack in the previous example: softmax ( dense ( relu ( dense ( input , w1 , b1 ) ) , w2 , b2 ) ) The chain structure shown here is the most common structure used in neural networks. You can consider each function $f^{(n)}$ as a layer in the neural network - for example $f^{(2)} is the 2nd layer in the network. The number of function calls in the structure is the depth of the network. This is where the term deep learning comes from. Neural networks are often written as the mapping: $$y = f(x; \\theta)$$ Where $x$ is the input to the neural network and $\\theta$ are the set of learned parameters. In Elixir, you would write this: y = model ( input , params ) From the previous example, params would represent the collection: { w1 , b1 , w2 , b2 } where w1 and w2 are layer kernels , and b1 and b2 are layer biases .","ref":"Axon.Layers.html","title":"Axon.Layers","type":"module"},{"doc":"Functional implementation of general dimensional adaptive average pooling. Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform average pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#adaptive_avg_pool/2","title":"Axon.Layers.adaptive_avg_pool/2","type":"function"},{"doc":"Functional implementation of general dimensional adaptive power average pooling. Computes: $$f(X) = qrt[p]{ um_{x in X} x^{p}}$$ Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform max pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :norm - $p$ from above equation. Defaults to 2. :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required.","ref":"Axon.Layers.html#adaptive_lp_pool/2","title":"Axon.Layers.adaptive_lp_pool/2","type":"function"},{"doc":"Functional implementation of general dimensional adaptive max pooling. Adaptive pooling allows you to specify the desired output size of the transformed input. This will automatically adapt the window size and strides to obtain the desired output size. It will then perform max pooling using the calculated window size and strides. Adaptive pooling can be useful when working on multiple inputs with different spatial input shapes. You can guarantee the output of an adaptive pooling operation is always the same size regardless of input shape. Options :output_size - spatial output size. Must be a tuple with size equal to the spatial dimensions in the input tensor. Required.","ref":"Axon.Layers.html#adaptive_max_pool/2","title":"Axon.Layers.adaptive_max_pool/2","type":"function"},{"doc":"Functional implementation of an alpha dropout layer. Alpha dropout is a type of dropout that forces the input to have zero mean and unit standard deviation. Randomly masks some elements and scales to enforce self-normalization. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor . References Self-Normalizing Neural Networks","ref":"Axon.Layers.html#alpha_dropout/2","title":"Axon.Layers.alpha_dropout/2","type":"function"},{"doc":"A general dimensional functional average pooling layer. Pooling is applied to the spatial dimension of the input tensor. Average pooling returns the average of all elements in valid windows in the input tensor. It is often used after convolutional layers to downsample the input even further. Options kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#avg_pool/2","title":"Axon.Layers.avg_pool/2","type":"function"},{"doc":"Functional implementation of batch normalization. Normalizes the input by calculating mean and variance of the input tensor along every dimension but the given :channel_index , and then scaling according to: $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$$ gamma and beta are often trainable parameters. If training? is true, this method will compute a new mean and variance, and return the updated ra_mean and ra_var . Otherwise, it will just compute batch norm from the given ra_mean and ra_var. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation. :momentum - momentum to use for EMA update. :training? - if true, uses training mode batch norm. Defaults to false. References Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","ref":"Axon.Layers.html#batch_norm/6","title":"Axon.Layers.batch_norm/6","type":"function"},{"doc":"Functional implementation of a bilinear layer. Bilinear transformation of the input such that: $$y = x_1^{T}Ax_2 + b$$ Parameter Shapes input1 - {batch_size, ..., input1_features} input2 - {batch_size, ..., input2_features} kernel - {out_features, input1_features, input2_features} Output Shape {batch_size, ..., output_features} Examples iex&gt; inp1 = Nx . iota ( { 3 , 2 } , type : { :f , 32 } ) iex&gt; inp2 = Nx . iota ( { 3 , 4 } , type : { :f , 32 } ) iex&gt; kernel = Nx . iota ( { 1 , 2 , 4 } , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( 1.0 ) iex&gt; Axon.Layers . bilinear ( inp1 , inp2 , kernel , bias ) # Nx.Tensor &lt; f32 [ 3 ] [ 1 ] [ [ 39.0 ] , [ 455.0 ] , [ 1319.0 ] ] &gt;","ref":"Axon.Layers.html#bilinear/5","title":"Axon.Layers.bilinear/5","type":"function"},{"doc":"","ref":"Axon.Layers.html#celu/2","title":"Axon.Layers.celu/2","type":"function"},{"doc":"Functional implementation of a general dimensional convolutional layer. Convolutional layers can be described as applying a convolution over an input signal composed of several input planes. Intuitively, the input kernel slides output_channels number of filters over the input tensor to extract features from the input tensor. Convolutional layers are most commonly used in computer vision, but can also be useful when working with sequences and other input signals. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} kernel - {output_channels, input_channels, kernel_spatial0, ..., kernel_spatialN} bias - {} or {output_channels} Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples One-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ 0.1294 , - 0.6638 , 1.0251 ] ] , [ [ 0.9182 , 1.1512 , - 1.6149 ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ [ [ - 1.5475 , 1.2425 ] ] , [ [ 0.1871 , 0.5458 ] ] , [ [ - 0.4488 , 0.8879 ] ] ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 0.7791 , 0.1676 , 1.5971 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 2 ] [ 3 ] [ 2 ] [ [ [ - 0.24591797590255737 , 3.08001708984375 ] , [ - 0.1704912781715393 , 0.6029025316238403 ] , [ 0.9496372938156128 , 2.80519962310791 ] ] , [ [ 0.7885514497756958 , - 3.0088953971862793 ] , [ 0.9677201509475708 , - 0.4984228312969208 ] , [ 2.207162380218506 , - 0.3534282445907593 ] ] ] &gt; Two-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ [ - 1.0476 , - 0.5041 ] , [ - 0.9336 , 1.5907 ] ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ ...&gt; [ [ [ 0.7514 , 0.7356 ] , [ 1.3909 , 0.6800 ] ] ] , ...&gt; [ [ [ - 0.3450 , 0.4551 ] , [ - 0.6275 , - 0.9875 ] ] ] , ...&gt; [ [ [ 1.8587 , 0.4722 ] , [ 0.6058 , - 1.0301 ] ] ] ...&gt; ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 1.9564 , 0.2822 , - 0.5385 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 0.5815491676330566 ] ] , [ [ - 0.5707762241363525 ] ] , [ [ - 4.927865028381348 ] ] ] ] &gt; Three-dimensional convolution iex&gt; input = Nx . tensor ( [ [ [ [ [ - 0.6497 ] , [ 1.0939 ] ] , [ [ - 2.5465 ] , [ 0.7801 ] ] ] ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ ...&gt; [ [ [ [ 0.7390 ] , [ - 0.0927 ] ] , [ [ - 0.8675 ] , [ - 0.9209 ] ] ] ] , ...&gt; [ [ [ [ - 0.6638 ] , [ 0.4341 ] ] , [ [ 0.6368 ] , [ 1.1846 ] ] ] ] ...&gt; ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ - 0.4101 , 0.1776 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . conv ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 1 ] [ 2 ] [ 1 ] [ 1 ] [ 1 ] [ [ [ [ [ 0.49906185269355774 ] ] ] , [ [ [ 0.38622811436653137 ] ] ] ] ] &gt;","ref":"Axon.Layers.html#conv/4","title":"Axon.Layers.conv/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#conv_lstm/6","title":"Axon.Layers.conv_lstm/6","type":"function"},{"doc":"Functional implementation of a general dimensional transposed convolutional layer. Note: This layer is currently implemented as a fractionally strided convolution by padding the input tensor. Please open an issue if you'd like this behavior changed. Transposed convolutions are sometimes (incorrectly) referred to as deconvolutions because it &quot;reverses&quot; the spatial dimensions of a normal convolution. Transposed convolutions are a form of upsampling - they produce larger spatial dimensions than the input tensor. They can be thought of as a convolution in reverse - and are sometimes implemented as the backward pass of a normal convolution. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; input = Nx . iota ( { 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; kernel = Nx . iota ( { 6 , 3 , 2 } , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( 1.0 , type : { :f , 32 } ) iex&gt; Axon.Layers . conv_transpose ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 1 ] [ 6 ] [ 4 ] [ [ [ 40.0 , 79.0 , 94.0 , 43.0 ] , [ 94.0 , 205.0 , 256.0 , 133.0 ] , [ 148.0 , 331.0 , 418.0 , 223.0 ] , [ 202.0 , 457.0 , 580.0 , 313.0 ] , [ 256.0 , 583.0 , 742.0 , 403.0 ] , [ 310.0 , 709.0 , 904.0 , 493.0 ] ] ] &gt; References A guide to convolution arithmetic for deep learning Deconvolutional Networks","ref":"Axon.Layers.html#conv_transpose/4","title":"Axon.Layers.conv_transpose/4","type":"function"},{"doc":"Functional implementation of a dense layer. Linear transformation of the input such that: $$y = xW^T + b$$ A dense layer or fully connected layer transforms the input using the given kernel matrix and bias to compute: Nx . dot ( input , kernel ) + bias Typically, both kernel and bias are learnable parameters trained using gradient-based optimization. Parameter Shapes input - {batch_size, * input_features} kernel - {input_features, output_features} bias - {} or {output_features} Output Shape {batch_size, *, output_features} Examples iex&gt; input = Nx . tensor ( [ [ 1.0 , 0.5 , 1.0 , 0.5 ] , [ 0.0 , 0.0 , 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; kernel = Nx . tensor ( [ [ 0.2 ] , [ 0.3 ] , [ 0.5 ] , [ 0.8 ] ] , type : { :f , 32 } ) iex&gt; bias = Nx . tensor ( [ 1.0 ] , type : { :f , 32 } ) iex&gt; Axon.Layers . dense ( input , kernel , bias ) # Nx.Tensor &lt; f32 [ 2 ] [ 1 ] [ [ 2.25 ] , [ 1.0 ] ] &gt;","ref":"Axon.Layers.html#dense/4","title":"Axon.Layers.dense/4","type":"function"},{"doc":"Functional implementation of a general dimensional depthwise convolution. Depthwise convolutions apply a single convolutional filter to each input channel. This is done by setting feature_group_size equal to the number of input channels. This will split the output_channels into input_channels number of groups and convolve the grouped kernel channels over the corresponding input channel. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} kernel - {output_channels, 1, kernel_spatial0, ..., kernel_spatialN} bias - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first .","ref":"Axon.Layers.html#depthwise_conv/4","title":"Axon.Layers.depthwise_conv/4","type":"function"},{"doc":"Functional implementation of a dropout layer. Applies a mask to some elements of the input tensor with probability rate and scales the input tensor by a factor of $\\frac{1}{1 - rate}$. Dropout is a form of regularization that helps prevent overfitting by preventing models from becoming too reliant on certain connections. Dropout can somewhat be thought of as learning an ensemble of models with random connections masked. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting mask across feature channels or other dimensions. Defaults to shape of input tensor. References Dropout: A Simple Way to Prevent Neural Networks from Overfitting","ref":"Axon.Layers.html#dropout/2","title":"Axon.Layers.dropout/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#elu/2","title":"Axon.Layers.elu/2","type":"function"},{"doc":"Computes embedding by treating kernel matrix as a lookup table for discrete tokens. input is a vector of discrete values, typically representing tokens (e.g. words, characters, etc.) from a vocabulary. kernel is a kernel matrix of shape {vocab_size, embedding_size} from which the dense embeddings will be drawn. Parameter Shapes input - {batch_size, ..., seq_len} kernel - {vocab_size, embedding_size} Examples iex&gt; input = Nx . tensor ( [ [ 1 , 2 , 4 , 5 ] , [ 4 , 3 , 2 , 9 ] ] ) iex&gt; kernels = Nx . tensor ( [ ...&gt; [ 0.46299999952316284 , 0.5562999844551086 , 0.18170000612735748 ] , ...&gt; [ 0.9801999926567078 , 0.09780000150203705 , 0.5333999991416931 ] , ...&gt; [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , ...&gt; [ 0.31929999589920044 , 0.42250001430511475 , 0.7865999937057495 ] , ...&gt; [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , ...&gt; [ 0.1898999959230423 , 0.9311000108718872 , 0.8356000185012817 ] , ...&gt; [ 0.6383000016212463 , 0.8794000148773193 , 0.5282999873161316 ] , ...&gt; [ 0.9523000121116638 , 0.7597000002861023 , 0.08250000327825546 ] , ...&gt; [ 0.6622999906539917 , 0.02329999953508377 , 0.8205999732017517 ] , ...&gt; [ 0.9855999946594238 , 0.36419999599456787 , 0.5372999906539917 ] ...&gt; ] ) iex&gt; Axon.Layers . embedding ( input , kernels ) # Nx.Tensor &lt; f32 [ 2 ] [ 4 ] [ 3 ] [ [ [ 0.9801999926567078 , 0.09780000150203705 , 0.5333999991416931 ] , [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , [ 0.1898999959230423 , 0.9311000108718872 , 0.8356000185012817 ] ] , [ [ 0.5519000291824341 , 0.5662999749183655 , 0.20559999346733093 ] , [ 0.31929999589920044 , 0.42250001430511475 , 0.7865999937057495 ] , [ 0.6980000138282776 , 0.9240999817848206 , 0.23479999601840973 ] , [ 0.9855999946594238 , 0.36419999599456787 , 0.5372999906539917 ] ] ] &gt;","ref":"Axon.Layers.html#embedding/3","title":"Axon.Layers.embedding/3","type":"function"},{"doc":"Functional implementation of a feature alpha dropout layer. Feature alpha dropout applies dropout in the same manner as spatial dropout; however, it also enforces self-normalization by masking inputs with the SELU activation function and scaling unmasked inputs. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor .","ref":"Axon.Layers.html#feature_alpha_dropout/2","title":"Axon.Layers.feature_alpha_dropout/2","type":"function"},{"doc":"Flattens input to shape of {batch, units} by folding outer dimensions. Examples iex&gt; Axon.Layers . flatten ( Nx . iota ( { 1 , 2 , 2 } , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 1 ] [ 4 ] [ [ 0.0 , 1.0 , 2.0 , 3.0 ] ] &gt;","ref":"Axon.Layers.html#flatten/2","title":"Axon.Layers.flatten/2","type":"function"},{"doc":"Functional implementation of global average pooling which averages across the spatial dimensions of the input such that the only remaining dimensions are the batch and feature dimensions. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, features, s1, ..., sN} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false Examples iex&gt; Axon.Layers . global_avg_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 1.0 , 4.0 ] , [ 7.0 , 10.0 ] , [ 13.0 , 16.0 ] ] &gt; iex&gt; Axon.Layers . global_avg_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 32 } ) , keep_axes : true ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 1.5 ] ] , [ [ 5.5 ] ] , [ [ 9.5 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_avg_pool/2","title":"Axon.Layers.global_avg_pool/2","type":"function"},{"doc":"Functional implementation of global LP pooling which computes the following function across spatial dimensions of the input: $$f(X) = qrt[p]{ um_{x in X} x^{p}}$$ Where $p$ is given by the keyword argument :norm . As $p$ approaches infinity, it becomes equivalent to max pooling. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, s1, ..., sN, features} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false :norm - $p$ in above function. Defaults to 2 Examples iex&gt; Axon.Layers . global_lp_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) , norm : 1 ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 3.0 , 12.0 ] , [ 21.0 , 30.0 ] , [ 39.0 , 48.0 ] ] &gt; iex&gt; Axon.Layers . global_lp_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 16 } ) , keep_axes : true ) # Nx.Tensor &lt; f16 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 3.7421875 ] ] , [ [ 11.2265625 ] ] , [ [ 19.125 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_lp_pool/2","title":"Axon.Layers.global_lp_pool/2","type":"function"},{"doc":"Functional implementation of global max pooling which computes maximums across the spatial dimensions of the input such that the only remaining dimensions are the batch and feature dimensions. Assumes data is configured in a channels-first like format. Parameter Shapes input - {batch_size, s1, ..., sN, features} Options :keep_axes - option to keep reduced axes with size 1 for each reduced dimensions. Defaults to false Examples iex&gt; Axon.Layers . global_max_pool ( Nx . iota ( { 3 , 2 , 3 } , type : { :f , 32 } ) ) # Nx.Tensor &lt; f32 [ 3 ] [ 2 ] [ [ 2.0 , 5.0 ] , [ 8.0 , 11.0 ] , [ 14.0 , 17.0 ] ] &gt; iex&gt; Axon.Layers . global_max_pool ( Nx . iota ( { 1 , 3 , 2 , 2 } , type : { :f , 32 } ) , keep_axes : true ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ 1 ] [ [ [ [ 3.0 ] ] , [ [ 7.0 ] ] , [ [ 11.0 ] ] ] ] &gt;","ref":"Axon.Layers.html#global_max_pool/2","title":"Axon.Layers.global_max_pool/2","type":"function"},{"doc":"Functional implementation of group normalization. Normalizes the input by reshaping input into groups of given :group_size and then calculating the mean and variance along every dimension but the input batch dimension. $$y = rac{x - E[x]}{ qrt{Var[x] + \epsilon}} * gamma + \beta$$ gamma and beta are often trainable parameters. This method does not maintain an EMA of mean and variance. Options :group_size - channel group size. Size of each group to split input channels into. :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes and group shape for mean and variance calculation. References Group Normalization","ref":"Axon.Layers.html#group_norm/4","title":"Axon.Layers.group_norm/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#gru/6","title":"Axon.Layers.gru/6","type":"function"},{"doc":"","ref":"Axon.Layers.html#hard_sigmoid/2","title":"Axon.Layers.hard_sigmoid/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#hard_silu/2","title":"Axon.Layers.hard_silu/2","type":"function"},{"doc":"Functional implementation of instance normalization. Normalizes the input by calculating mean and variance of the input tensor along the spatial dimensions of the input. $$y = rac{x - E[x]}{ qrt{Var[x] + \epsilon}} * gamma + \beta$$ gamma and beta are often trainable parameters. If training? is true, this method will compute a new mean and variance, and return the updated ra_mean and ra_var . Otherwise, it will just compute batch norm from the given ra_mean and ra_var. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation. :momentum - momentum to use for EMA update. :training? - if true, uses training mode batch norm. Defaults to false. References Instance Normalization: The Missing Ingredient for Fast Stylization","ref":"Axon.Layers.html#instance_norm/6","title":"Axon.Layers.instance_norm/6","type":"function"},{"doc":"Functional implementation of layer normalization. Normalizes the input by calculating mean and variance of the input tensor along the given feature dimension :channel_index . $$y = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$$ gamma and beta are often trainable parameters. This method does not maintain an EMA of mean and variance. Options :epsilon - numerical stability term. $epsilon$ in the above formulation. :channel_index - channel index used to determine reduction axes for mean and variance calculation.","ref":"Axon.Layers.html#layer_norm/4","title":"Axon.Layers.layer_norm/4","type":"function"},{"doc":"","ref":"Axon.Layers.html#leaky_relu/2","title":"Axon.Layers.leaky_relu/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#log_softmax/2","title":"Axon.Layers.log_softmax/2","type":"function"},{"doc":"Functional implementation of a general dimensional power average pooling layer. Pooling is applied to the spatial dimension of the input tensor. Power average pooling computes the following function on each valid window of the input tensor: $$f(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}$$ Where $p$ is given by the keyword argument :norm . As $p$ approaches infinity, it becomes equivalent to max pooling. Options :norm - $p$ from above equation. Defaults to 2. :kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to size of kernel. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; t = Nx . tensor ( [ [ [ 0.9450 , 0.4684 , 1.8146 ] , [ 1.2663 , 0.4354 , - 0.0781 ] , [ - 0.4759 , 0.3251 , 0.8742 ] ] ] , type : { :f , 32 } ) iex&gt; Axon.Layers . lp_pool ( t , kernel_size : 2 , norm : 2 ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ [ [ 1.0547149181365967 ] , [ 1.3390626907348633 ] , [ 0.5763426423072815 ] ] ] &gt;","ref":"Axon.Layers.html#lp_pool/2","title":"Axon.Layers.lp_pool/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#lstm/6","title":"Axon.Layers.lstm/6","type":"function"},{"doc":"Functional implementation of a general dimensional max pooling layer. Pooling is applied to the spatial dimension of the input tensor. Max pooling returns the maximum element in each valid window of the input tensor. It is often used after convolutional layers to downsample the input even further. Options kernel_size - window size. Rank must match spatial dimension of the input tensor. Required. :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to size of kernel. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :window_dilations - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Can be scalar or list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . Examples iex&gt; t = Nx . tensor ( [ [ ...&gt; [ 0.051500000059604645 , - 0.7042999863624573 , - 0.32899999618530273 ] , ...&gt; [ - 0.37130001187324524 , 1.6191999912261963 , - 0.11829999834299088 ] , ...&gt; [ 0.7099999785423279 , 0.7282999753952026 , - 0.18639999628067017 ] ] ] , type : { :f , 32 } ) iex&gt; Axon.Layers . max_pool ( t , kernel_size : 2 ) # Nx.Tensor &lt; f32 [ 1 ] [ 3 ] [ 1 ] [ [ [ 0.051500000059604645 ] , [ 1.6191999912261963 ] , [ 0.7282999753952026 ] ] ] &gt;","ref":"Axon.Layers.html#max_pool/2","title":"Axon.Layers.max_pool/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#multiply/2","title":"Axon.Layers.multiply/2","type":"function"},{"doc":"Resizes a batch of tensors to the given shape using one of a number of sampling methods. Requires input option :to which should be a tuple specifying the resized spatial dimensions of the input tensor. Input tensor must be at least rank 3, with fixed batch and channel dimensions. Resizing will upsample or downsample using the given resize method. Supported resize methods are :nearest, :linear, :bilinear, :trilinear, :cubic, :bicubic, :tricubic . Examples iex&gt; img = Nx . iota ( { 1 , 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , to : { 4 , 4 } ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ 4 ] [ 4 ] [ [ [ [ 0.0 , 1.0 , 1.0 , 2.0 ] , [ 3.0 , 4.0 , 4.0 , 5.0 ] , [ 3.0 , 4.0 , 4.0 , 5.0 ] , [ 6.0 , 7.0 , 7.0 , 8.0 ] ] ] ] &gt; iex&gt; img = Nx . iota ( { 1 , 1 , 3 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , to : { 2 } ) # Nx.Tensor &lt; f32 [ 1 ] [ 1 ] [ 2 ] [ [ [ 0.0 , 2.0 ] ] ] &gt; iex&gt; img = Nx . iota ( { 1 , 2 , 2 , 2 , 1 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , to : { 1 , 3 , 2 } ) # Nx.Tensor &lt; f32 [ 1 ] [ 2 ] [ 1 ] [ 3 ] [ 2 ] [ [ [ [ [ 2.0 , 2.0 ] , [ 3.0 , 3.0 ] , [ 3.0 , 3.0 ] ] ] , [ [ [ 6.0 , 6.0 ] , [ 7.0 , 7.0 ] , [ 7.0 , 7.0 ] ] ] ] ] &gt; Error cases iex&gt; img = Nx . iota ( { 1 , 1 , 3 , 3 } , type : { :f , 32 } ) iex&gt; Axon.Layers . resize ( img , to : { 4 , 4 } , method : :foo ) ** (ArgumentError) invalid resize method :foo, resize method must be one of :nearest","ref":"Axon.Layers.html#resize/2","title":"Axon.Layers.resize/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#selu/2","title":"Axon.Layers.selu/2","type":"function"},{"doc":"Functional implementation of a 2-dimensional separable depthwise convolution. The 2-d depthwise separable convolution performs 2 depthwise convolutions each over 1 spatial dimension of the input. Parameter Shapes input - {batch_size, input_channels, input_spatial0, ..., input_spatialN} k1 - {output_channels, 1, kernel_spatial0, 1} b1 - {output_channels} or {} k2 - {output_channels, 1, 1, kernel_spatial1} b2 - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . References Xception: Deep Learning with Depthwise Separable Convolutions","ref":"Axon.Layers.html#separable_conv2d/6","title":"Axon.Layers.separable_conv2d/6","type":"function"},{"doc":"Functional implementation of a 3-dimensional separable depthwise convolution. The 3-d depthwise separable convolution performs 3 depthwise convolutions each over 1 spatial dimension of the input. Parameter Shapes input - {batch_size, input_channels, input_spatial0, input_spatial1, input_spatial2} k1 - {output_channels, 1, kernel_spatial0, 1, 1} b1 - {output_channels} or {} k2 - {output_channels, 1, 1, kernel_spatial1, 1} b2 - {output_channels} or {} k3 - {output_channels, 1, 1, 1, 1, kernel_spatial2} b3 - {output_channels} or {} output_channels must be a multiple of the input channels. Options :strides - kernel strides. Can be a scalar or a list who's length matches the number of spatial dimensions in the input tensor. Defaults to 1. :padding - zero padding on the input. Can be one of :valid , :same or a general padding configuration without interior padding for each spatial dimension of the input. :input_dilation - input dilation factor. Equivalent to applying interior padding on the input. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :kernel_dilation - kernel dilation factor. Equivalent to applying interior padding on the kernel. The amount of interior padding applied is given by kernel_dilation - 1 . Defaults to 1 or no dilation. :channels - channel configuration. One of :first or :last . Defaults to :first . References Xception: Deep Learning with Depthwise Separable Convolutions","ref":"Axon.Layers.html#separable_conv3d/8","title":"Axon.Layers.separable_conv3d/8","type":"function"},{"doc":"","ref":"Axon.Layers.html#softmax/2","title":"Axon.Layers.softmax/2","type":"function"},{"doc":"Functional implementation of an n-dimensional spatial dropout layer. Applies a mask to entire feature maps instead of individual elements. This is done by calculating a mask shape equal to the spatial dimensions of the input tensor with 1 channel, and then broadcasting the mask across the feature dimension of the input tensor. Options :rate - dropout rate. Used to determine probability a connection will be dropped. Required. # :noise_shape - input noise shape. Shape of mask which can be useful for broadcasting ` mask ` across feature channels or other dimensions . Defaults to shape of input tensor . References Efficient Object Localization Using Convolutional Networks","ref":"Axon.Layers.html#spatial_dropout/2","title":"Axon.Layers.spatial_dropout/2","type":"function"},{"doc":"","ref":"Axon.Layers.html#subtract/2","title":"Axon.Layers.subtract/2","type":"function"},{"doc":"Loss functions. Loss functions evaluate predictions with respect to true data, often to measure the divergence between a model's representation of the data-generating distribution and the true representation of the data-generating distribution. Each loss function is implemented as an element-wise function measuring the loss with respect to the input target y_true and input prediction y_pred . As an example, the mean_squared_error/2 loss function produces a tensor whose values are the mean squared error between targets and predictions: iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; It's common to compute the loss across an entire minibatch. You can easily do so by specifying a :reduction mode, or by composing one of these with an Nx reduction method: iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; You can even compose loss functions: defn my_strange_loss ( y_true , y_pred ) do y_true |&gt; Axon.Losses . mean_squared_error ( y_pred ) |&gt; Axon.Losses . binary_cross_entropy ( y_pred ) |&gt; Nx . sum ( ) end Or, more commonly, you can combine loss functions with penalties for regularization: defn regularized_loss ( params , y_true , y_pred ) do loss = Axon . mean_squared_error ( y_true , y_pred ) penalty = l2_penalty ( params ) Nx . sum ( loss ) + penalty end All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Losses.html","title":"Axon.Losses","type":"module"},{"doc":"Binary cross-entropy loss function. $$l_i = -\\frac{1}{2}(\\hat{y_i} \\cdot \\log(y_i) + (1 - \\hat{y_i}) \\cdot \\log(1 - y_i))$$ Binary cross-entropy loss is most often used in binary classification problems. By default, it expects y_pred to encode probabilities from [0.0, 1.0] , typically as the output of the sigmoid function or another function which squeezes values between 0 and 1. You may optionally set from_logits: true to specify that values are being sent as non-normalized values (e.g. weights with possibly infinite range). In this case, input values will be encoded as probabilities by applying the logistic sigmoid function before computing loss. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . :negative_weights - class weight for 0 class useful for scaling loss by importance of class. Defaults to 1.0 . :positive_weights - class weight for 1 class useful for scaling loss by importance of class. Defaults to 1.0 . :from_logits - whether y_pred is a logits tensor. Defaults to false . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.8644826412200928 , 0.5150600075721741 , 0.45986634492874146 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.613136351108551 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6811 , 0.5565 ] , [ 0.6551 , 0.4551 ] , [ 0.5422 , 0.2648 ] ] ) iex&gt; Axon.Losses . binary_cross_entropy ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.8394089937210083 &gt;","ref":"Axon.Losses.html#binary_cross_entropy/3","title":"Axon.Losses.binary_cross_entropy/3","type":"function"},{"doc":"Categorical cross-entropy loss function. $$l_i = -\\sum_i^C \\hat{y_i} \\cdot \\log(y_i)$$ Categorical cross-entropy is typically used for multi-class classifcation problems. By default, it expects y_pred to encode a probability distribution along the last axis. You can specify from_logits: true to indicate y_pred is a logits tensor. # Batch size of 3 with 3 target classes y_true = Nx . tensor ( [ 0 , 2 , 1 ] ) y_pred = Nx . tensor ( [ [ 0.2 , 0.8 , 0.0 ] , [ 0.1 , 0.2 , 0.7 ] , [ 0.1 , 0.2 , 0.7 ] ] ) Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . :class_weights - 1-D list corresponding to weight of each class useful for scaling loss according to importance of class. Tensor size must match number of classes in dataset. Defaults to 1.0 for all classes. :from_logits - whether y_pred is a logits tensor. Defaults to false . :sparse - whether y_true encodes a &quot;sparse&quot; tensor. In this case the inputs are integer values corresponding to the target class. Defaults to false . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.051293306052684784 , 2.3025851249694824 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 1.1769392490386963 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 2.3538784980773926 &gt; iex&gt; y_true = Nx . tensor ( [ 1 , 2 ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05 , 0.95 , 0 ] , [ 0.1 , 0.8 , 0.1 ] ] ) iex&gt; Axon.Losses . categorical_cross_entropy ( y_true , y_pred , reduction : :sum , sparse : true ) # Nx.Tensor &lt; f32 2.3538784980773926 &gt;","ref":"Axon.Losses.html#categorical_cross_entropy/3","title":"Axon.Losses.categorical_cross_entropy/3","type":"function"},{"doc":"Categorical hinge loss function. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 1.6334158182144165 , 1.2410175800323486 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 1.4372167587280273 &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.05300799 , 0.21617081 , 0.68642382 ] , [ 0.3754382 , 0.08494169 , 0.13442067 ] ] ) iex&gt; Axon.Losses . categorical_hinge ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 2.8744335174560547 &gt;","ref":"Axon.Losses.html#categorical_hinge/3","title":"Axon.Losses.categorical_hinge/3","type":"function"},{"doc":"Connectionist Temporal Classification loss. Argument Shapes l_true - $(B)$ y_true - $(B, S)$ y_pred - $(B, T, D)$ Options :reduction - reduction mode. One of :sum or :none . Defaults to :none . Description l_true contains lengths of target sequences. Nonzero positive values. y_true contains target sequences. Each value represents a class of element in range of available classes 0 &lt;= y &lt; D. Blank element class is included in this range, but shouldn't be presented among y_true values. Maximum target sequence length should be lower or equal to y_pred sequence length: S &lt;= T. y_pred - log probabilities of classes D along the prediction sequence T.","ref":"Axon.Losses.html#connectionist_temporal_classification/3","title":"Axon.Losses.connectionist_temporal_classification/3","type":"function"},{"doc":"Hinge loss function. $$\\frac{1}{C}\\max_i(1 - \\hat{y_i} * y_i, 0)$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.9700339436531067 , 0.6437881588935852 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.806911051273346 &gt; iex&gt; y_true = Nx . tensor ( [ [ 1 , 1 , - 1 ] , [ 1 , 1 , - 1 ] ] , type : { :s , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.45440044 , 0.31470688 , 0.67920924 ] , [ 0.24311459 , 0.93466766 , 0.10914676 ] ] ) iex&gt; Axon.Losses . hinge ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.613822102546692 &gt;","ref":"Axon.Losses.html#hinge/3","title":"Axon.Losses.hinge/3","type":"function"},{"doc":"Kullback-Leibler divergence loss function. $$l_i = \\sum_i^C \\hat{y_i} \\cdot \\log(\\frac{\\hat{y_i}}{y_i})$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.916289210319519 , - 3.080907390540233e-6 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.45814305543899536 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 0 , 0 ] ] , type : { :u , 8 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.6 , 0.4 ] , [ 0.4 , 0.6 ] ] ) iex&gt; Axon.Losses . kl_divergence ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.9162861108779907 &gt;","ref":"Axon.Losses.html#kl_divergence/3","title":"Axon.Losses.kl_divergence/3","type":"function"},{"doc":"Logarithmic-Hyperbolic Cosine loss function. $$l_i = \\frac{1}{C} \\sum_i^C (\\hat{y_i} - y_i) + \\log(1 + e^{-2(\\hat{y_i} - y_i)}) - \\log(2)$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.2168903946876526 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.1084451973438263 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] ) iex&gt; Axon.Losses . log_cosh ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.2168903946876526 &gt;","ref":"Axon.Losses.html#log_cosh/3","title":"Axon.Losses.log_cosh/3","type":"function"},{"doc":"Margin ranking loss function. $$l_i = \\max(0, -\\hat{y_i} * (y^(1)_i - y^(2)_i) + \\alpha)$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.0 , 0.9909000396728516 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } , reduction : :mean ) # Nx.Tensor &lt; f32 0.3303000032901764 &gt; iex&gt; y_true = Nx . tensor ( [ 1.0 , 1.0 , 1.0 ] , type : { :f , 32 } ) iex&gt; y_pred1 = Nx . tensor ( [ 0.6934 , - 0.7239 , 1.1954 ] , type : { :f , 32 } ) iex&gt; y_pred2 = Nx . tensor ( [ - 0.4691 , 0.2670 , - 1.7452 ] , type : { :f , 32 } ) iex&gt; Axon.Losses . margin_ranking ( y_true , { y_pred1 , y_pred2 } , reduction : :sum ) # Nx.Tensor &lt; f32 0.9909000396728516 &gt;","ref":"Axon.Losses.html#margin_ranking/3","title":"Axon.Losses.margin_ranking/3","type":"function"},{"doc":"Mean-absolute error loss function. $$l_i = \\sum_i |\\hat{y_i} - y_i|$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_absolute_error ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.0 &gt;","ref":"Axon.Losses.html#mean_absolute_error/3","title":"Axon.Losses.mean_absolute_error/3","type":"function"},{"doc":"Mean-squared error loss function. $$l_i = \\sum_i (\\hat{y_i} - y_i)^2$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.5 , 0.5 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.5 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . mean_squared_error ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.0 &gt;","ref":"Axon.Losses.html#mean_squared_error/3","title":"Axon.Losses.mean_squared_error/3","type":"function"},{"doc":"Poisson loss function. $$l_i = \\frac{1}{C} \\sum_i^C y_i - (\\hat{y_i} \\cdot \\log(y_i))$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 2 ] [ 0.9999999403953552 , 0.0 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.4999999701976776 &gt; iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . poisson ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 0.9999999403953552 &gt;","ref":"Axon.Losses.html#poisson/3","title":"Axon.Losses.poisson/3","type":"function"},{"doc":"Soft margin loss function. $$l_i = \\sum_i \\frac{\\log(1 + e^{-\\hat{y_i} * y_i})}{N}$$ Options :reduction - reduction mode. One of :mean , :sum , or :none . Defaults to :none . Examples iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred ) # Nx.Tensor &lt; f32 [ 3 ] [ 0.851658046245575 , 0.7822436094284058 , 0.3273470401763916 ] &gt; iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred , reduction : :mean ) # Nx.Tensor &lt; f32 0.6537495255470276 &gt; iex&gt; y_true = Nx . tensor ( [ [ - 1.0 , 1.0 , 1.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 0.2953 , - 0.1709 , 0.9486 ] ] , type : { :f , 32 } ) iex&gt; Axon.Losses . soft_margin ( y_true , y_pred , reduction : :sum ) # Nx.Tensor &lt; f32 1.9612486362457275 &gt;","ref":"Axon.Losses.html#soft_margin/3","title":"Axon.Losses.soft_margin/3","type":"function"},{"doc":"Metric functions. Metrics are used to measure the performance and compare performance of models in easy-to-understand terms. Often times, neural networks use surrogate loss functions such as negative log-likelihood to indirectly optimize a certain performance metric. Metrics such as accuracy, also called the 0-1 loss, do not have useful derivatives (e.g. they are information sparse), and are often intractable even with low input dimensions. Despite not being able to train specifically for certain metrics, it's still useful to track these metrics to monitor the performance of a neural network during training. Metrics such as accuracy provide useful feedback during training, whereas loss can sometimes be difficult to interpret. You can attach any of these functions as metrics within the Axon.Loop API using Axon.Loop.metric/3 . All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Metrics.html","title":"Axon.Metrics","type":"module"},{"doc":"Computes the accuracy of the given predictions. If the size of the last axis is 1, it performs a binary accuracy computation with a threshold of 0.5. Otherwise, computes categorical accuracy. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 1 ] , [ 0 ] , [ 0 ] ] ) , Nx . tensor ( [ [ 1 ] , [ 1 ] , [ 1 ] ] ) ) # Nx.Tensor &lt; f32 0.3333333432674408 &gt; iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) , Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 0 , 1 ] ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt; iex&gt; Axon.Metrics . accuracy ( Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] ] ) , Nx . tensor ( [ [ 0 , 1 , 0 ] , [ 0 , 1 , 0 ] ] ) ) # Nx.Tensor &lt; f32 0.5 &gt;","ref":"Axon.Metrics.html#accuracy/2","title":"Axon.Metrics.accuracy/2","type":"function"},{"doc":"Computes the number of false negative predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . false_negatives ( y_true , y_pred ) # Nx.Tensor &lt; u64 3 &gt;","ref":"Axon.Metrics.html#false_negatives/3","title":"Axon.Metrics.false_negatives/3","type":"function"},{"doc":"Computes the number of false positive predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . false_positives ( y_true , y_pred ) # Nx.Tensor &lt; u64 2 &gt;","ref":"Axon.Metrics.html#false_positives/3","title":"Axon.Metrics.false_positives/3","type":"function"},{"doc":"Calculates the mean absolute error of predictions with respect to targets. $$l_i = \\sum_i |\\hat{y_i} - y_i|$$ Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Examples iex&gt; y_true = Nx . tensor ( [ [ 0.0 , 1.0 ] , [ 0.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; y_pred = Nx . tensor ( [ [ 1.0 , 1.0 ] , [ 1.0 , 0.0 ] ] , type : { :f , 32 } ) iex&gt; Axon.Metrics . mean_absolute_error ( y_true , y_pred ) # Nx.Tensor &lt; f32 0.5 &gt;","ref":"Axon.Metrics.html#mean_absolute_error/2","title":"Axon.Metrics.mean_absolute_error/2","type":"function"},{"doc":"Computes the precision of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . precision ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#precision/3","title":"Axon.Metrics.precision/3","type":"function"},{"doc":"Computes the recall of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . recall ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#recall/3","title":"Axon.Metrics.recall/3","type":"function"},{"doc":"Returns a function which computes a running average given current average, new observation, and current iteration. Examples iex&gt; cur_avg = 0.5 iex&gt; iteration = 1 iex&gt; y_true = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; y_pred = Nx . tensor ( [ [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 0 ] ] ) iex&gt; avg_acc = Axon.Metrics . running_average ( &amp; Axon.Metrics . accuracy / 2 ) iex&gt; avg_acc . ( cur_avg , [ y_true , y_pred ] , iteration ) # Nx.Tensor &lt; f32 0.75 &gt;","ref":"Axon.Metrics.html#running_average/1","title":"Axon.Metrics.running_average/1","type":"function"},{"doc":"Returns a function which computes a running sum given current sum, new observation, and current iteration. Examples iex&gt; cur_sum = 12 iex&gt; iteration = 2 iex&gt; y_true = Nx . tensor ( [ 0 , 1 , 0 , 1 ] ) iex&gt; y_pred = Nx . tensor ( [ 1 , 1 , 0 , 1 ] ) iex&gt; fps = Axon.Metrics . running_sum ( &amp; Axon.Metrics . false_positives / 2 ) iex&gt; fps . ( cur_sum , [ y_true , y_pred ] , iteration ) # Nx.Tensor &lt; s64 13 &gt;","ref":"Axon.Metrics.html#running_sum/1","title":"Axon.Metrics.running_sum/1","type":"function"},{"doc":"Computes the sensitivity of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . sensitivity ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.6666666865348816 &gt;","ref":"Axon.Metrics.html#sensitivity/3","title":"Axon.Metrics.sensitivity/3","type":"function"},{"doc":"Computes the specificity of the given predictions with respect to the given targets. Argument Shapes y_true - $(d_0, d_1, ..., d_n)$ y_pred - $(d_0, d_1, ..., d_n)$ Options :threshold - threshold for truth value of the predictions. Defaults to 0.5 Examples iex&gt; Axon.Metrics . specificity ( Nx . tensor ( [ 0 , 1 , 1 , 1 ] ) , Nx . tensor ( [ 1 , 0 , 1 , 1 ] ) ) # Nx.Tensor &lt; f32 0.0 &gt;","ref":"Axon.Metrics.html#specificity/3","title":"Axon.Metrics.specificity/3","type":"function"},{"doc":"Computes the number of true negative predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . true_negatives ( y_true , y_pred ) # Nx.Tensor &lt; u64 1 &gt;","ref":"Axon.Metrics.html#true_negatives/3","title":"Axon.Metrics.true_negatives/3","type":"function"},{"doc":"Computes the number of true positive predictions with respect to given targets. Options :threshold - threshold for truth value of predictions. Defaults to 0.5 . Examples iex&gt; y_true = Nx . tensor ( [ 1 , 0 , 1 , 1 , 0 , 1 , 0 ] ) iex&gt; y_pred = Nx . tensor ( [ 0.8 , 0.6 , 0.4 , 0.2 , 0.8 , 0.2 , 0.2 ] ) iex&gt; Axon.Metrics . true_positives ( y_true , y_pred ) # Nx.Tensor &lt; u64 1 &gt;","ref":"Axon.Metrics.html#true_positives/3","title":"Axon.Metrics.true_positives/3","type":"function"},{"doc":"Functional implementations of common recurrent neural network routines. Recurrent Neural Networks are commonly used for working with sequences of data where there is some level of dependence between outputs at different timesteps. This module contains 3 RNN Cell functions and methods to &quot;unroll&quot; cells over an entire sequence. Each cell function returns a tuple: { new_carry , output } Where new_carry is an updated carry state and output is the output for a singular timestep. In order to apply an RNN across multiple timesteps, you need to use either static_unroll or dynamic_unroll (coming soon). Unrolling an RNN is equivalent to a map_reduce or scan starting from an initial carry state and ending with a final carry state and an output sequence. All of the functions in this module are implemented as numerical functions and can be JIT or AOT compiled with any supported Nx compiler.","ref":"Axon.Recurrent.html","title":"Axon.Recurrent","type":"module"},{"doc":"ConvLSTM Cell. When combined with Axon.Recurrent.*_unroll , implements a ConvLSTM-based RNN. More memory efficient than traditional LSTM. Options :strides - convolution strides. Defaults to 1 . :padding - convolution padding. Defaults to :same . References Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting","ref":"Axon.Recurrent.html#conv_lstm_cell/6","title":"Axon.Recurrent.conv_lstm_cell/6","type":"function"},{"doc":"Dynamically unrolls an RNN. Unrolls implement a scan operation which applies a transformation on the leading axis of input_sequence carrying some state. In this instance cell_fn is an RNN cell function such as lstm_cell or gru_cell . This function will make use of an defn while-loop such and thus may be more efficient for long sequences.","ref":"Axon.Recurrent.html#dynamic_unroll/6","title":"Axon.Recurrent.dynamic_unroll/6","type":"function"},{"doc":"GRU Cell. When combined with Axon.Recurrent.*_unroll , implements a GRU-based RNN. More memory efficient than traditional LSTM. References Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling","ref":"Axon.Recurrent.html#gru_cell/7","title":"Axon.Recurrent.gru_cell/7","type":"function"},{"doc":"LSTM Cell. When combined with Axon.Recurrent.*_unroll , implements a LSTM-based RNN. More memory efficient than traditional LSTM. References Long Short-Term Memory","ref":"Axon.Recurrent.html#lstm_cell/7","title":"Axon.Recurrent.lstm_cell/7","type":"function"},{"doc":"Statically unrolls an RNN. Unrolls implement a scan operation which applies a transformation on the leading axis of input_sequence carrying some state. In this instance cell_fn is an RNN cell function such as lstm_cell or gru_cell . This function inlines the unrolling of the sequence such that the entire operation appears as a part of the compilation graph. This makes it suitable for shorter sequences.","ref":"Axon.Recurrent.html#static_unroll/6","title":"Axon.Recurrent.static_unroll/6","type":"function"},{"doc":"Implementations of common gradient-based optimization algorithms. All of the methods in this module are written in terms of the update methods defined in Axon.Updates . Axon treats optimizers as the tuple: { init_fn , update_fn } where init_fn returns an initial optimizer state and update_fn scales input gradients. init_fn accepts a model's parameters and attaches state to each parameter. update_fn accepts gradients, optimizer state, and current model parameters and returns updated optimizer state and gradients. Custom optimizers are often created via the Axon.Updates API. Example Consider the following usage of the Adam optimizer in a basic update function (assuming objective and the dataset are defined elsewhere): defmodule Learning do import Nx.Defn defn init ( params , init_fn ) do init_fn . ( params ) end defn update ( params , optimizer_state , inputs , targets , update_fn ) do { loss , gradient } = value_and_grad ( params , &amp; objective ( &amp;1 , inputs , targets ) ) { scaled_updates , new_optimizer_state } = update_fn . ( gradient , optimizer_state , params ) { Axon.Updates . apply_updates ( params , scaled_updates ) , new_optimizer_state , loss } end end model_params = Nx . random_uniform ( { 784 , 10 } ) { init_fn , update_fn } = Axon.Optimizers . adam ( 0.005 ) optimizer_state = Learning . init ( params , init_fn ) { new_params , new_optimizer_state , loss } = Learning . update ( params , optimizer_state , inputs , targets , update_fn ) For a simpler approach, you can also use optimizers with the training API: model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , Axon.Optimizers . adam ( 0.005 ) ) |&gt; Axon.Loop . run ( data , epochs : 10 , compiler : EXLA )","ref":"Axon.Optimizers.html","title":"Axon.Optimizers","type":"module"},{"doc":"Adabelief optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 0.0 :eps_root - numerical stability term. Defaults to 1.0e-16 References AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients","ref":"Axon.Optimizers.html#adabelief/2","title":"Axon.Optimizers.adabelief/2","type":"function"},{"doc":"Adagrad optimizer. Options :eps - numerical stability term. Defaults to 1.0e-7 References Adaptive Subgradient Methods for Online Learning and Stochastic Optimization","ref":"Axon.Optimizers.html#adagrad/2","title":"Axon.Optimizers.adagrad/2","type":"function"},{"doc":"Adam optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 1.0e-15 References Adam: A Method for Stochastic Optimization","ref":"Axon.Optimizers.html#adam/2","title":"Axon.Optimizers.adam/2","type":"function"},{"doc":"Adam with weight decay optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :decay - weight decay. Defaults to 0.0","ref":"Axon.Optimizers.html#adamw/2","title":"Axon.Optimizers.adamw/2","type":"function"},{"doc":"Fromage optimizer. Options :min_norm - minimum norm value. Defaults to 0.0 . References On the distance between two neural networks and the stability of learning","ref":"Axon.Optimizers.html#fromage/2","title":"Axon.Optimizers.fromage/2","type":"function"},{"doc":"Lamb optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :decay - weight decay. Defaults to 0.0 :min_norm - minimum norm value. Defaults to 0.0 References Large Batch Optimization for Deep Learning: Training BERT in 76 minutes","ref":"Axon.Optimizers.html#lamb/2","title":"Axon.Optimizers.lamb/2","type":"function"},{"doc":"Noisy SGD optimizer. Options :eta - used to compute variance of noise distribution. Defaults to 0.1 :gamma - used to compute variance of noise distribution. Defaults to 0.55","ref":"Axon.Optimizers.html#noisy_sgd/2","title":"Axon.Optimizers.noisy_sgd/2","type":"function"},{"doc":"Rectified Adam optimizer. Options :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 :threshold - threshold term. Defaults to 5.0 References On the Variance of Adaptive Learning Rate and Beyond","ref":"Axon.Optimizers.html#radam/2","title":"Axon.Optimizers.radam/2","type":"function"},{"doc":"RMSProp optimizer. Options :centered - whether to scale by centered root of EMA of squares. Defaults to false :momentum - momentum term. If set, uses SGD with momentum and decay set to value of this term. :nesterov - whether or not to use nesterov momentum. Defaults to false :initial_scale - initial value of EMA. Defaults to 0.0 :decay - EMA decay rate. Defaults to 0.9 :eps - numerical stability term. Defaults to 1.0e-8","ref":"Axon.Optimizers.html#rmsprop/2","title":"Axon.Optimizers.rmsprop/2","type":"function"},{"doc":"SGD optimizer. Options :momentum - momentum term. If set, uses SGD with momentum and decay set to value of this term. :nesterov - whether or not to use nesterov momentum. Defaults to false","ref":"Axon.Optimizers.html#sgd/2","title":"Axon.Optimizers.sgd/2","type":"function"},{"doc":"Yogi optimizer. Options :initial_accumulator_value - initial value for first and second moment. Defaults to 0.0 :b1 - first moment decay. Defaults to 0.9 :b2 - second moment decay. Defaults to 0.999 :eps - numerical stability term. Defaults to 1.0e-8 :eps_root - numerical stability term. Defaults to 0.0 References Adaptive Methods for Nonconvex Optimization","ref":"Axon.Optimizers.html#yogi/2","title":"Axon.Optimizers.yogi/2","type":"function"},{"doc":"Parameter update methods. Update methods transform the input tensor in some way, usually by scaling or shifting the input with respect to some input state. Update methods are composed to create more advanced optimization methods such as AdaGrad or Adam. Each update returns a tuple: { init_fn , update_fn } Which represent a state initialization and state update function respectively. While each method in the Updates API is a regular Elixir function, the two methods they return are implemented as defn , so they can be accelerated using any Nx backend or compiler. Update methods are just combinators that can be arbitrarily composed to create complex optimizers. For example, the Adam optimizer in Axon.Optimizers is implemented as: def adam ( learning_rate , opts \\\\ [ ] ) do Updates . scale_by_adam ( opts ) |&gt; Updates . scale ( - learning_rate ) end Updates are maps of updates, often associated with parameters of the same names. Using Axon.Updates.apply_updates/3 will merge updates and parameters by adding associated parameters and updates, and ensuring any given model state is preserved. Custom combinators You can create your own combinators using the stateless/2 and stateful/3 primitives. Every update method in this module is implemented in terms of one of these two primitives. stateless/2 represents a stateless update: def scale ( combinator \\\\ Axon.Updates . identity ( ) , step_size ) do stateless ( combinator , &amp; apply_scale ( &amp;1 , &amp;2 , step_size ) ) end defnp apply_scale ( x , _params , step ) do transform ( { x , step } , fn { updates , step } -&gt; deep_new ( updates , fn x -&gt; Nx . multiply ( x , step ) end ) end ) end Notice how the function given to stateless/2 is defined within defn . This is what allows the anonymous functions returned by Axon.Updates to be used inside defn . stateful/3 represents a stateful update and follows the same pattern: def my_stateful_update ( updates ) do Axon.Updates . stateful ( updates , &amp; init_my_update / 1 , &amp; apply_my_update / 2 ) end defnp init_my_update ( params ) do state = zeros_like ( params ) %{ state : state } end defnp apply_my_update ( updates , state ) do new_state = deep_new ( state , fn v -&gt; Nx . add ( v , 0.01 ) end ) updates = transform ( { updates , new_state } , fn { updates , state } -&gt; deep_merge ( updates , state , fn g , z -&gt; Nx . multiply ( g , z ) end ) end ) { updates , %{ state : new_state } } end State associated with individual parameters should have keys that match the keys of the parameter. For example, if you have parameters %{kernel: kernel} with associated states mu and nu representing the first and second moments, your state should look something like: %{ mu : %{ kernel : kernel_mu } nu : %{ kernel : kernel_nu } }","ref":"Axon.Updates.html","title":"Axon.Updates","type":"module"},{"doc":"Adds decayed weights to updates. Commonly used as a regularization strategy. Options * ` :decay ` - Rate of decay . Defaults to ` 0.0 ` .","ref":"Axon.Updates.html#add_decayed_weights/1","title":"Axon.Updates.add_decayed_weights/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#add_decayed_weights/2","title":"Axon.Updates.add_decayed_weights/2","type":"function"},{"doc":"Adds random Gaussian noise to the input. Options * ` :eta ` - Controls amount of noise to add . Defaults to ` 0.01 ` . * ` :gamma ` - Controls amount of noise to add . Defaults to ` 0.55 ` .","ref":"Axon.Updates.html#add_noise/1","title":"Axon.Updates.add_noise/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#add_noise/2","title":"Axon.Updates.add_noise/2","type":"function"},{"doc":"Applies updates to params and updates state parameters with given state map.","ref":"Axon.Updates.html#apply_updates/3","title":"Axon.Updates.apply_updates/3","type":"function"},{"doc":"Centralizes input by shifting updates by their mean.","ref":"Axon.Updates.html#centralize/1","title":"Axon.Updates.centralize/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#centralize/2","title":"Axon.Updates.centralize/2","type":"function"},{"doc":"Clips input between -delta and delta. Options :delta - maximum absolute value of the input. Defaults to 2.0","ref":"Axon.Updates.html#clip/1","title":"Axon.Updates.clip/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#clip/2","title":"Axon.Updates.clip/2","type":"function"},{"doc":"Clips input using input global norm. Options :max_norm - maximum norm value of input. Defaults to 1.0","ref":"Axon.Updates.html#clip_by_global_norm/1","title":"Axon.Updates.clip_by_global_norm/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#clip_by_global_norm/2","title":"Axon.Updates.clip_by_global_norm/2","type":"function"},{"doc":"Composes two updates. This is useful for extending optimizers without having to reimplement them. For example, you can implement gradient centralization: import Axon.Updates Axon.Updates . compose ( Axon.Updates . centralize ( ) , Axon.Optimizers . rmsprop ( ) ) This is equivalent to: Axon.Updates . centralize ( ) |&gt; Axon.Updates . scale_by_rms ( )","ref":"Axon.Updates.html#compose/2","title":"Axon.Updates.compose/2","type":"function"},{"doc":"Returns the identity update. This is often as the initial update in many functions in this module.","ref":"Axon.Updates.html#identity/0","title":"Axon.Updates.identity/0","type":"function"},{"doc":"","ref":"Axon.Updates.html#identity/1","title":"Axon.Updates.identity/1","type":"function"},{"doc":"Scales input by a fixed step size. $$f(x_i) = \\alpha x_i$$","ref":"Axon.Updates.html#scale/2","title":"Axon.Updates.scale/2","type":"function"},{"doc":"Scales input according to Adam algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 1.0e-15 ` References Adam: A Method for Stochastic Optimization","ref":"Axon.Updates.html#scale_by_adam/1","title":"Axon.Updates.scale_by_adam/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_adam/2","title":"Axon.Updates.scale_by_adam/2","type":"function"},{"doc":"Scales input according to the AdaBelief algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` . * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` . * ` :eps ` - numerical stability term . Defaults to ` 0.0 ` . * ` :eps_root ` - numerical stability term . Defaults to ` 1.0e-16 ` . References AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients","ref":"Axon.Updates.html#scale_by_belief/1","title":"Axon.Updates.scale_by_belief/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_belief/2","title":"Axon.Updates.scale_by_belief/2","type":"function"},{"doc":"Scale input according to the Rectified Adam algorithm. Options * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 0.0 ` * ` :threshold ` - threshold for variance . Defaults to ` 5.0 ` References On the Variance of the Adaptive Learning Rate and Beyond","ref":"Axon.Updates.html#scale_by_radam/1","title":"Axon.Updates.scale_by_radam/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_radam/2","title":"Axon.Updates.scale_by_radam/2","type":"function"},{"doc":"Scales input by the root of the EMA of squared inputs. Options * ` :decay ` - EMA decay rate . Defaults to ` 0.9 ` . * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` . References Overview of mini-batch gradient descent","ref":"Axon.Updates.html#scale_by_rms/1","title":"Axon.Updates.scale_by_rms/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_rms/2","title":"Axon.Updates.scale_by_rms/2","type":"function"},{"doc":"Scales input by the root of all prior squared inputs. Options * ` :eps ` - numerical stability term . Defaults to ` 1.0e-7 `","ref":"Axon.Updates.html#scale_by_rss/1","title":"Axon.Updates.scale_by_rss/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_rss/2","title":"Axon.Updates.scale_by_rss/2","type":"function"},{"doc":"Scales input using the given schedule function. This can be useful for implementing learning rate schedules. The number of update iterations is tracked by an internal counter. You might need to update the schedule to operate on per-batch schedule rather than per-epoch.","ref":"Axon.Updates.html#scale_by_schedule/2","title":"Axon.Updates.scale_by_schedule/2","type":"function"},{"doc":"Scales input by the root of the centered EMA of squared inputs. Options * ` :decay ` - EMA decay rate . Defaults to ` 0.9 ` . * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` . References Overview of mini-batch gradient descent","ref":"Axon.Updates.html#scale_by_stddev/1","title":"Axon.Updates.scale_by_stddev/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_stddev/2","title":"Axon.Updates.scale_by_stddev/2","type":"function"},{"doc":"Scale by trust ratio. Options * ` :min_norm ` - Min norm to clip . Defaults to ` 0.0 ` . * ` :trust_coefficient ` - Trust coefficient . Defaults to ` 1.0 ` . * ` :eps ` - Numerical stability term . Defaults to ` 0.0 ` .","ref":"Axon.Updates.html#scale_by_trust_ratio/1","title":"Axon.Updates.scale_by_trust_ratio/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_trust_ratio/2","title":"Axon.Updates.scale_by_trust_ratio/2","type":"function"},{"doc":"Scale input according to the Yogi algorithm. Options * ` :initial_accumulator_value ` - Initial state accumulator value . * ` :b1 ` - first moment decay . Defaults to ` 0.9 ` * ` :b2 ` - second moment decay . Defaults to ` 0.999 ` * ` :eps ` - numerical stability term . Defaults to ` 1.0e-8 ` * ` :eps_root ` - numerical stability term . Defaults to ` 0.0 ` References * [ Adaptive Methods for Nonconvex Optimization ] ( https :// proceedings . neurips . cc / paper / 2018 / file / 90365351 ccc7437a1309dc64e4db32a3 - Paper . pdf )","ref":"Axon.Updates.html#scale_by_yogi/1","title":"Axon.Updates.scale_by_yogi/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#scale_by_yogi/2","title":"Axon.Updates.scale_by_yogi/2","type":"function"},{"doc":"Represents a stateful update. Stateful updates require some update state, such as momentum or RMS of previous updates. Therefore you must implement some initialization function as well as an update function.","ref":"Axon.Updates.html#stateful/3","title":"Axon.Updates.stateful/3","type":"function"},{"doc":"Represents a stateless update. Stateless updates do not depend on an update state and thus only require an implementation of an update function.","ref":"Axon.Updates.html#stateless/2","title":"Axon.Updates.stateless/2","type":"function"},{"doc":"Trace inputs with past inputs. Options :decay - decay rate for tracing past updates. Defaults to 0.9 :nesterov - whether to use Nesterov momentum. Defaults to false","ref":"Axon.Updates.html#trace/1","title":"Axon.Updates.trace/1","type":"function"},{"doc":"","ref":"Axon.Updates.html#trace/2","title":"Axon.Updates.trace/2","type":"function"},{"doc":"Abstraction for modeling a reduction of a dataset with an accumulated state for a number of epochs. Inspired heavily by PyTorch Ignite . The main abstraction is the %Loop{} struct, which controls a nested reduction of the form: Enum . reduce ( 1 .. max_epochs , state , fn epoch , state -&gt; Enum . reduce ( data , state , &amp; batch_step / 2 ) end ) data is assumed to be an Enumerable or Stream of input data which is handled by a processing function, batch_step . The purpose of the loop abstraction is to take away much of the boilerplate used in solving machine learning tasks. Tasks such as normalizing a dataset, hyperparameter optimization, or training machine learning models boil down to writing one function: defn batch_step ( batch , state ) do # ...do something with batch... updated_state end For tasks such as training a neural network, state will encapsulate things such as model and optimizer state. For supervised learning tasks, batch_step might look something like: defn batch_step ( { inputs , targets } , state ) do %{ parameters : params , optimizer_state : optim_state } = state gradients = grad ( params , objective_fn . ( &amp;1 , inputs , targets ) ) { updates , new_optim_state } = optimizer . ( optim_state , params , gradients ) new_params = apply_updates ( params , updates ) %{ parameters : new_params , optimizer_state : optim_state } end batch_step takes a batch of {input, target} pairs and the current state, and updates the model parameters based on the gradients received from some arbitrary objective function. This function will run in a nested loop, iterating over the entire dataset for N epochs before finally returning the trained model state. By defining 1 function, we've created a training loop that works for most machine learning models. In actuality, the loop abstraction accumulates a struct, Axon.Loop.State , which looks like (assuming container is a generic Elixir container of tensors, e.g. map, tuple, etc.): % State { epoch : integer ( ) , max_epoch : integer ( ) , iteration : integer ( ) , max_iteration : integer ( ) , metrics : map ( string ( ) , container ( ) ) , times : map ( integer ( ) , integer ( ) ) , step_state : container ( ) } batch_step takes in the batch and the step state field and returns a step_state , which is a generic container of state accumulated at each iteration. The rest of the fields in the state struct are updated automatically behind the scenes. The loop must start from some initial step state, thus most tasks must also provide an additional initialization function to provide some starting point for the step state. For machine learning tasks, the initialization function will return things like initial model parameters and optimizer state. Typically, the final output of the loop is the accumulated final state; however, you may optionally apply an output transform to extract specific values at the end of the loop. For example, Axon.Loop.trainer/4 by default extracts trained model state: output_transform = fn state -&gt; state . step_state [ :model_state ] end Initialize and Step The core of the Axon loop are the init and step functions. The initialization is an arity-0 function which provides an initial step state: init = fn -&gt; %{ params : Axon . init ( model ) } end While the step function is the batch_step function mentioned earlier: step = fn data , state -&gt; new_state = # ...do something... new_state end Metrics Often times you want to compute metrics associated with your training iterations. To accomplish this, you can attach metrics to each Axon.Loop . Assuming a batch_step function which looks like: defn batch_step ( { inputs , targets } , state ) do %{ parameters : params , optimizer_state : optim_state } = state gradients = grad ( params , objective_fn . ( &amp;1 , inputs , targets ) ) { updates , new_optim_state } = optimizer . ( optim_state , params , gradients ) new_params = apply_updates ( params , updates ) # Shown for simplicity, you can optimize this by calculating preds # along with the gradient calculation preds = model_fn . ( params , inputs ) %{ y_true : targets , y_pred : preds , parameters : new_params , optimizer_state : optim_state } end You can attach metrics to this by using Axon.Loop.metric/4 : Axon.Loop . loop ( &amp; batch_step / 2 ) |&gt; Axon.Loop . metric ( &quot;Accuracy&quot; , :accuracy , fn %{ y_true : y_ , y_pred : y } -&gt; [ y_ , y ] end ) |&gt; Axon.Loop . run ( data ) Because metrics work directly on step_state , you typically need to provide an output transform to indicate which values should be passed to your metric function. By default, Axon assumes a supervised training task with the fields :y_true and :y_pred present in the step state. See Axon.Loop.metric/4 for more information. Metrics will be tracked in the loop state using the user-provided key. Metrics integrate seamlessly with the supervised metrics defined in Axon.Metrics . You can also use metrics to keep running averages of some values in the original dataset. Events and Handlers You can instrument several points in the loop using event handlers. By default, several events are fired when running a loop: events = [ :started , # After loop state initialization :epoch_started , # On epoch start :iteration_started , # On iteration start :iteration_completed , # On iteration complete :epoch_completed , # On epoch complete :epoch_halted , # On epoch halt, if early halted :halted , # On loop halt, if early halted :completed # On loop completion ] You can attach event handlers to events using Axon.Loop.handle/4 : loop |&gt; Axon.Loop . handle ( :iteration_completed , &amp; log_metrics / 1 , every : 100 ) |&gt; Axon.Loop . run ( data ) The above will trigger log_metrics/1 every 100 times the :iteration_completed event is fired. Event handlers must return a tuple {status, state} , where status is an atom with one of the following values: :continue # Continue epoch, continue looping :halt_epoch # Halt the epoch, continue looping :halt_loop # Halt looping And state is an updated Axon.Loop.State struct. Handler functions take as input the current loop state. It's important to note that event handlers are triggered in the order they are attached to the loop. If you have two handlers on the same event, they will trigger in order: loop |&gt; Axon.Loop . handle ( :epoch_completed , &amp; normalize_state / 1 ) # Runs first |&gt; Axon.Loop . handle ( :epoch_completed , &amp; log_state / 1 ) # Runs second You may provide filters to filter when event handlers trigger. See Axon.Loop.handle/4 for more details on valid filters. Factories Axon loops are typically created from one of the factory functions provided in this module: * ` Axon.Loop . loop / 3 ` - Creates a loop from step function and optional initialization functions and output transform functions . * ` Axon.Loop . trainer / 3 ` - Creates a supervised training loop from model , loss , and optimizer . * ` Axon.Loop . evaluator / 2 ` - Creates a supervised evaluator loop from model and model state . Running loops In order to execute a loop, you should use Axon.Loop.run/3 : loop |&gt; Axon.Loop . run ( data , epochs : 10 ) Resuming loops At times you may want to resume a loop from some previous state. You can accomplish this with Axon.Loop.from_state/2 : loop |&gt; Axon.Loop . from_state ( state ) |&gt; Axon.Loop . run ( data )","ref":"Axon.Loop.html","title":"Axon.Loop","type":"module"},{"doc":"Adds a handler function which saves loop checkpoints on a given event, optionally with metric-based criteria. By default, loop checkpoints will be saved at the end of every epoch in the current working directory under the checkpoint/ path. Checkpoints are serialized representations of loop state obtained from Axon.Loop.serialize_state/2 . Serialization options will be forwarded to Axon.Loop.serialize_state/2 . You can customize checkpoint events by passing :event and :filter options: loop |&gt; Axon.Loop . checkpoint ( event : :iteration_completed , filter : [ every : 50 ] ) Checkpoints are saved under the checkpoint/ directory with a pattern of checkpoint_{epoch}.ckpt . You can customize the path and pattern with the :path and :file_pattern options: my_file_pattern = fn % Axon.Loop.State { epoch : epoch , iteration : iter } -&gt; &quot;checkpoint_ \#{ epoch } _ \#{ iter } &quot; end loop |&gt; Axon.Loop . checkpoint ( path : &quot;my_checkpoints&quot; , file_pattern : my_file_pattern ) If you'd like to only save checkpoints based on some metric criteria, you can specify the :criteria option. :criteria must be a valid key in metrics: loop |&gt; Axon.Loop . checkpoint ( criteria : &quot;validation_loss&quot; ) The default criteria mode is :min , meaning the min score metric will be considered &quot;best&quot; when deciding to save on a given event. Valid modes are :min and :max : loop |&gt; Axon.Loop . checkpoint ( criteria : &quot;validation_accuracy&quot; , mode : :max )","ref":"Axon.Loop.html#checkpoint/2","title":"Axon.Loop.checkpoint/2","type":"function"},{"doc":"Deserializes loop state from a binary. It is the opposite of Axon.Loop.serialize_state/2 . By default, the step state is deserialized using Nx.deserialize.2 ; however, this behavior can be changed if step state is an application specific container. For example, if you introduce your own data structure into step_state and you customized the serialization logic, Nx.deserialize/2 will not be sufficient for deserialization. - you must pass custom logic with :deserialize_step_state .","ref":"Axon.Loop.html#deserialize_state/2","title":"Axon.Loop.deserialize_state/2","type":"function"},{"doc":"Adds a handler function which halts a loop if the given metric does not improve between events. By default, this will run after each epoch and track the improvement of a given metric. You must specify a metric to monitor and the metric must be present in the loop state. Typically, this will be a validation metric: model |&gt; Axon.Loop . trainer ( loss , optim ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . validate ( val_data ) |&gt; Axon.Loop . early_stop ( &quot;validation_accuracy&quot; ) It's important to remember that handlers are executed in the order they are added to the loop. For example, if you'd like to checkpoint a loop after every epoch and use early stopping, most likely you want to add the checkpoint handler before the early stopping handler: model |&gt; Axon.Loop . trainer ( loss , optim ) |&gt; Axon.Loop . metric ( :accuracy ) |&gt; Axon.Loop . checkpoint ( ) |&gt; Axon.Loop . early_stop ( &quot;accuracy&quot; ) That will ensure checkpoint is always fired, even if the loop exited early.","ref":"Axon.Loop.html#early_stop/3","title":"Axon.Loop.early_stop/3","type":"function"},{"doc":"Creates a supervised evaluation step from a model and model state. This function is intended for more fine-grained control over the loop creation process. It returns a tuple of {init_fn, step_fn} where init_fn returns an initial step state and step_fn performs a single evaluation step.","ref":"Axon.Loop.html#eval_step/1","title":"Axon.Loop.eval_step/1","type":"function"},{"doc":"Creates a supervised evaluator from a model and model state. An evaluator can be used for things such as testing and validation of models after or during training. It assumes model is an Axon struct, container of structs, or a tuple of init / apply functions. model_state must be a container usable from within model . The evaluator returns a step state of the form: %{ y_true : labels , y_pred : predictions } Such that you can attach any number of supervised metrics to the evaluation loop: model |&gt; Axon.Loop . evaluator ( trained_state ) |&gt; Axon.Loop . metric ( &quot;Accuracy&quot; , :accuracy ) Applies an output transform which returns the map of metrics accumulated over the given loop.","ref":"Axon.Loop.html#evaluator/1","title":"Axon.Loop.evaluator/1","type":"function"},{"doc":"Attaches state to the given loop in order to resume looping from a previous state. It's important to note that a loop's attached state takes precedence over defined initialization functions. Given initialization function: defn init_state ( ) , do : %{ foo : 1 , bar : 2 } And an attached state: state = % State { step_state : %{ foo : 2 , bar : 3 } } init_state/0 will never execute, and instead the initial step state of %{foo: 2, bar: 3} will be used.","ref":"Axon.Loop.html#from_state/2","title":"Axon.Loop.from_state/2","type":"function"},{"doc":"Adds a handler function to the loop which will be triggered on event with an optional filter. Events take place at different points during loop execution. The default events are: events = [ :started , # After loop state initialization :epoch_started , # On epoch start :iteration_started , # On iteration start :iteration_completed , # On iteration complete :epoch_completed , # On epoch complete :epoch_halted , # On epoch halt, if early halted :halted , # On loop halt, if early halted :completed # On loop completion ] Generally, event handlers are side-effecting operations which provide some sort of inspection into the loop's progress. It's important to note that if you define multiple handlers to be triggered on the same event, they will execute in order from when they were attached to the training loop: loop |&gt; Axon.Loop . handle ( :epoch_started , &amp; normalize_step_state / 1 ) # executes first |&gt; Axon.Loop . handle ( :epoch_started , &amp; log_step_state / 1 ) # executes second Thus, if you have separate handlers which alter or depend on loop state, you need to ensure they are ordered correctly, or combined into a single event handler for maximum control over execution. event must be an atom representing the event to trigger handler or a list of atoms indicating handler should be triggered on multiple events. event may be :all which indicates the handler should be triggered on every event during loop processing. handler must be an arity-1 function which takes as input loop state and returns {status, state} , where status is an atom with one of the following values: :continue # Continue epoch, continue looping :halt_epoch # Halt the epoch, continue looping :halt_loop # Halt looping filter is an atom representing a valid filter predicate, a keyword of predicate-value pairs, or a function which takes loop state and returns a true , indicating the handler should run, or false , indicating the handler should not run. Valid predicates are: :always # Always trigger event :once # Trigger on first event firing Valid predicate-value pairs are: every : N # Trigger every `N` event only : N # Trigger on `N` event","ref":"Axon.Loop.html#handle/4","title":"Axon.Loop.handle/4","type":"function"},{"doc":"Adds a handler function which logs the given message produced by message_fn to the given IO device every event satisfying filter . In most cases, this is useful for inspecting the contents of the loop state at intermediate stages. For example, the default trainer loop factory attaches IO logging of epoch, batch, loss and metrics. It's also possible to log loop state to files by changing the given IO device. By default, the IO device is :stdio . message_fn should take the loop state and return a binary representing the message to be written to the IO device.","ref":"Axon.Loop.html#log/5","title":"Axon.Loop.log/5","type":"function"},{"doc":"Creates a loop from step_fn , an optional init_fn , and an optional output_transform . step_fn is an arity-2 function which takes a batch and state and returns an updated step state: defn batch_step ( batch , step_state ) do step_state + 1 end init_fn by default is an identity function which forwards its initial arguments as the model state. You should define a custom initialization function if you require a different behavior: defn init_step_state ( state ) do Map . merge ( %{ foo : 1 } , state ) end You may use state in conjunction with initialization functions in init_fn . For example, train_step/3 uses initial state as initial model parameters to allow initializing models from partial parameterizations. step_batch/2 and init_step_state/1 are typically called from within Nx.Defn.jit/3 . While JIT-compilation will work with anonymous functions, def , and defn , it is recommended that you use the stricter defn to define both functions in order to avoid bugs or cryptic errors. output_transform/1 applies a transformation on the final accumulated loop state. This is useful for extracting specific fields from a loop and piping them into additional functions.","ref":"Axon.Loop.html#loop/3","title":"Axon.Loop.loop/3","type":"function"},{"doc":"Adds a metric of the given name to the loop. A metric is a function which tracks or measures some value with respect to values in the step state. For example, when training classification models, it's common to track the model's accuracy during training: loop |&gt; Axon.Loop . metric ( :accuracy , &quot;Accuracy&quot; ) By default, metrics assume a supervised learning task and extract the fields [:y_true, :y_pred] from the step state. If you wish to work on a different value, you can use an output transform. An output transform is a list of keys to extract from the output state, or a function which returns a flattened list of values to pass to the given metric function. Values received from output transforms are passed to the given metric using: value = output_transform . ( step_state ) apply ( metric , value ) Thus, even if you want your metric to work on a container, your output transform must return a list. metric must be an atom which matches the name of a metric in Axon.Metrics , or an arbitrary function which returns a tensor or container. name must be a string or atom used to store the computed metric in the loop state. If names conflict, the last attached metric will take precedence: loop |&gt; Axon.Loop . metric ( :mean_squared_error , &quot;Error&quot; ) # Will be overwritten |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;Error&quot; ) # Will be used By default, metrics keep a running average of the metric calculation. You can override this behavior by changing accumulate : loop |&gt; Axon.Loop . metric ( :true_negatives , &quot;tn&quot; , :running_sum ) Accumulation function can be one of the accumulation combinators in Axon.Metrics or an arity-3 function of the form: accumulate(acc, obs, i) :: new_acc .","ref":"Axon.Loop.html#metric/5","title":"Axon.Loop.metric/5","type":"function"},{"doc":"Runs the given loop on data with the given options. loop must be a valid Axon.Loop struct built from one of the loop factories provided in this module. data must be an Enumerable or Stream which yields batches of data on each iteration. Options :epochs - max epochs to run loop for. Must be non-negative integer. Defaults to 1 . :iterations - max iterations to run each epoch. Must be non-negative integer. Defaults to -1 or no max iterations. :jit_compile? - whether or not to JIT compile initialization and step functions. JIT compilation must be used for gradient computations. Defaults to true. Additional options are forwarded to Nx.Defn.jit as JIT-options. If no JIT options are set, the default options set with Nx.Defn.default_options are used.","ref":"Axon.Loop.html#run/4","title":"Axon.Loop.run/4","type":"function"},{"doc":"Serializes loop state to a binary for saving and loading loop from previous states. You can consider the serialized state to be a checkpoint of all state at a given iteration and epoch. By default, the step state is serialized using Nx.serialize/2 ; however, this behavior can be changed if step state is an application specific container. For example, if you introduce your own data structure into step_state, Nx.serialize/2 will not be sufficient for serialization - you must pass custom serialization as an option with :serialize_step_state . Additional opts controls serialization options such as compression. It is forwarded to :erlang.term_to_binary/2 .","ref":"Axon.Loop.html#serialize_state/2","title":"Axon.Loop.serialize_state/2","type":"function"},{"doc":"Creates a supervised train step from a model, loss function, and optimizer. This function is intended for more fine-grained control over the loop creation process. It returns a tuple of {init_fn, step_fn} where init_fn is an initialization function which returns an initial step state and step_fn is a supervised train step constructed from model , loss , and optimizer . model must be an Axon struct, a valid defn container of Axon structs, or a {init_fn, apply_fn} -tuple where init_fn is an arity-1 function which initializes the model state and apply_fn is an arity-2 function which applies the forward pass of the model. The forward pass of the model must return a map with keys :prediction and :state representing the model's prediction and updated state for layers which aggregate state during training. loss must be an atom which matches a function in Axon.Losses , a list of {loss, weight} tuples representing a basic weighted loss function for multi-output models, or an arity-2 function representing a custom loss function. optimizer must be an atom matching the name of a valid optimizer in Axon.Optimizers , or a {init_fn, update_fn} tuple where init_fn is an arity-1 function which initializes the optimizer state from attached parameters and update_fn is an arity-3 function which scales gradient updates with respect to input parameters, optimizer state, and gradients. See Axon.Updates for more information on building optimizers.","ref":"Axon.Loop.html#train_step/3","title":"Axon.Loop.train_step/3","type":"function"},{"doc":"Creates a supervised training loop from a model, loss function, and optimizer. This function is useful for training models on most standard supervised learning tasks. It assumes data consists of tuples of input-target pairs, e.g. [{x0, y0}, {x1, y1}, ..., {xN, yN}] where x0 and y0 are batched tensors or containers of batched tensors. It defines an initialization function which first initializes model state using the given model and then initializes optimizer state using the initial model state. The step function uses a differentiable objective function defined with respect to the model parameters, input data, and target data using the given loss function. It then updates model parameters using the given optimizer in order to minimize loss with respect to the model parameters. model must be an Axon struct, a valid defn container of Axon structs, or a {init_fn, apply_fn} -tuple where init_fn is an arity-1 function which initializes the model state and apply_fn is an arity-2 function which applies the forward pass of the model. loss must be an atom which matches a function in Axon.Losses , a list of {loss, weight} tuples representing a basic weighted loss function for multi-output models, or an arity-2 function representing a custom loss function. optimizer must be an atom matching the name of a valid optimizer in Axon.Optimizers , or a {init_fn, update_fn} tuple where init_fn is an arity-1 function which initializes the optimizer state from attached parameters and update_fn is an arity-3 function which scales gradient updates with respect to input parameters, optimizer state, and gradients. See Axon.Updates for more information on building optimizers. This function creates a step function which outputs a map consisting of the following fields for step_state : %{ y_pred : tensor ( ) | container ( tensor ( ) ) , # Model predictions for use in metrics y_true : tensor ( ) | container ( tensor ( ) ) , # True labels for use in metrics loss : tensor ( ) , # Running average of loss over epoch model_state : container ( tensor ( ) ) , # Model parameters and state optimizer_state : container ( tensor ( ) ) # Optimizer state associated with each parameter } Examples Basic usage data = Stream . zip ( input , target ) model = Axon . input ( { nil , 32 } , &quot;input&quot; ) |&gt; Axon . dense ( 1 , activation : :sigmoid ) model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , :adam ) |&gt; Axon.Loop . run ( data ) Customizing Optimizer model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , Axon.Optimizers . adam ( 0.05 ) ) |&gt; Axon.Loop . run ( data ) Custom loss loss_fn = fn y_true , y_pred -&gt; Nx . cos ( y_true , y_pred ) end model |&gt; Axon.Loop . trainer ( loss_fn , Axon.Optimizers . rmsprop ( 0.01 ) ) |&gt; Axon.Loop . run ( data ) Multiple objectives with multi-output model model = { Axon . input ( { nil , 1 } , &quot;input_0&quot; ) , Axon . input ( { nil , 2 } , &quot;input_1&quot; ) } loss_weights = [ mean_squared_error : 0.5 , mean_absolute_error : 0.5 ] model |&gt; Axon.Loop . trainer ( loss_weights ) |&gt; Axon.Loop . run ( data ) Options :log - training loss and metric log interval. Set to 0 to silence training logs. Defaults to 50","ref":"Axon.Loop.html#trainer/4","title":"Axon.Loop.trainer/4","type":"function"},{"doc":"Adds a handler function which tests the performance of model against the given validation set. This handler assumes the loop state matches the state initialized in a supervised training loop. Typically, you'd call this immediately after creating a supervised training loop: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . validate ( model , validation_data ) Please note that you must pass the same (or an equivalent) model into this method so it can be used during the validation loop. The metrics which are computed are those which are present BEFORE the validation handler was added to the loop. For the following loop: model |&gt; Axon.Loop . trainer ( :mean_squared_error , :sgd ) |&gt; Axon.Loop . metric ( :mean_absolute_error ) |&gt; Axon.Loop . validate ( model , validation_data ) |&gt; Axon.Loop . metric ( :binary_cross_entropy ) only :mean_absolute_error will be computed at validation time. The returned loop state is altered to contain validation metrics for use in later handlers such as early stopping and model checkpoints. Since the order of execution of event handlers is in the same order they are declared in the training loop, you MUST call this method before any other handler which expects or may use validation metrics.","ref":"Axon.Loop.html#validate/3","title":"Axon.Loop.validate/3","type":"function"},{"doc":"Accumulated state in an Axon.Loop. Loop state is a struct: % State { epoch : integer ( ) , max_epoch : integer ( ) , iteration : integer ( ) , max_iteration : integer ( ) , metrics : map ( string ( ) , container ( ) ) , times : map ( integer ( ) , integer ( ) ) , step_state : container ( ) , handler_metadata : container ( ) } epoch is the current epoch, starting at 0, of the nested loop. Defaults to 0. max_epoch is the maximum number of epochs the loop should run for. Defaults to 1. iteration is the current iteration of the inner loop. In supervised settings, this will be the current batch. Defaults to 0. max_iteration is the maximum number of iterations the loop should run a given epoch for. Defaults to -1 (no max). metrics is a map of %{&quot;metric_name&quot; =&gt; value} which accumulates metrics over the course of loop processing. Defaults to an empty map. times is a map of %{epoch_number =&gt; value} which maps a given epoch to the processing time. Defaults to an empty map. step_state is the step state as defined by the loop's processing initialization and update functions. step_state is a required field. handler_metadata is a metadata field for storing loop handler metadata. For example, loop checkpoints with specific metric criteria can store previous best metrics in the handler meta for use between iterations.","ref":"Axon.Loop.State.html","title":"Axon.Loop.State","type":"module"},{"doc":"","ref":"mnist.html","title":"MNIST","type":"extras"},{"doc":"This livebook will walk you through training a basic neural network using Axon, accelerated by the EXLA compiler. We'll be working on the MNIST dataset which is a dataset of handwritten digits with corresponding labels. The goal is to train a model that correctly classifies these handwritten digits with a single label [0-9].","ref":"mnist.html#introduction","title":"MNIST - Introduction","type":"extras"},{"doc":"First, we'll need to install our dependencies using Mix.install . We'll need Axon and it's dependencies, as well as the Req library for downloading the dataset. Mix . install ( [ { :req , &quot;~&gt; 0.3.0-dev&quot; , github : &quot;wojtekmach/req&quot; , branch : &quot;main&quot; } , { :axon , &quot;~&gt; 0.1.0-dev&quot; , github : &quot;elixir-nx/axon&quot; , branch : &quot;main&quot; } , { :exla , &quot;~&gt; 0.2.2&quot; } , { :nx , &quot;~&gt; 0.2.1&quot; } ] )","ref":"mnist.html#dependencies","title":"MNIST - Dependencies","type":"extras"},{"doc":"The MNIST dataset is available for free online. Using Req we'll download both training images and training labels. Both train_images and train_labels are compressed binary data. Fortunately, Req takes care of the decompression for us. You can read more about the format of the ubyte files here . Each file starts with a magic number and some metadata. We can use binary pattern matching to extract the information we want. In this case we extract the raw binary images and labels. base_url = &quot;https://storage.googleapis.com/cvdf-datasets/mnist/&quot; %{ body : train_images } = Req . get! ( base_url &lt;&gt; &quot;train-images-idx3-ubyte.gz&quot; ) %{ body : train_labels } = Req . get! ( base_url &lt;&gt; &quot;train-labels-idx1-ubyte.gz&quot; ) &lt;&lt; _ :: 32 , n_images :: 32 , n_rows :: 32 , n_cols :: 32 , images :: binary &gt;&gt; = train_images &lt;&lt; _ :: 32 , n_labels :: 32 , labels :: binary &gt;&gt; = train_labels We can easily read that binary data into a tensor using Nx.from_binary/2 . Nx.from_binary/2 expects a raw binary and a data type. In this case, both images and labels are stored as unsigned 8-bit integers. We can start by parsing our images: images = images |&gt; Nx . from_binary ( { :u , 8 } ) |&gt; Nx . reshape ( { n_images , 1 , n_rows , n_cols } , names : [ :images , :channels , :height , :width ] ) |&gt; Nx . divide ( 255 ) Nx.from_binary/2 returns a flat tensor. Using Nx.reshape/3 we can manipulate this flat tensor into meaningful dimensions. Notice we also normalized the tensor by dividing the input data by 255. This squeezes the data between 0 and 1 which often leads to better behavior when training models. Now, let's see what these images look like: images [ [ images : 0 .. 4 ] ] |&gt; Nx . to_heatmap ( ) In the reshape operation above, we give each dimension of the tensor a name. This makes it much easier to do things like slicing, and helps make your code easier to understand. Here we slice the images dimension of the images tensor to obtain the first 5 training images. Then, we convert them to a heatmap for easy visualization. It's common to train neural networks in batches (actually correctly called minibatches, but you'll see batch and minibatch used interchangeably). We can &quot;batch&quot; our images into batches of 32 like this: images = images |&gt; Nx . to_batched_list ( 32 ) Now, we'll need to get our labels into batches as well, but first we need to one-hot encode the labels. One-hot encoding converts input data from labels such as 3 , 5 , 7 , etc. into vectors of 0's and a single 1 at the correct labels index. As an example, a label of: 3 gets converted to: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] . targets = labels |&gt; Nx . from_binary ( { :u , 8 } ) |&gt; Nx . new_axis ( - 1 ) |&gt; Nx . equal ( Nx . tensor ( Enum . to_list ( 0 .. 9 ) ) ) |&gt; Nx . to_batched_list ( 32 )","ref":"mnist.html#retrieving-and-exploring-dataset","title":"MNIST - Retrieving and Exploring Dataset","type":"extras"},{"doc":"Let's start by defining a simple model: model = Axon . input ( { nil , 1 , 28 , 28 } , &quot;input&quot; ) |&gt; Axon . flatten ( ) |&gt; Axon . dense ( 128 , activation : :relu ) |&gt; Axon . dense ( 10 , activation : :softmax ) All Axon models start with an input layer to tell subsequent layers what shapes to expect. We then use Axon.flatten/2 which flattens the previous layer by squeezing all dimensions but the first dimension into a single dimension. Our model consists of 2 fully connected layers with 128 and 10 units respectively. The first layer uses :relu activation which returns max(0, input) element-wise. The final layer uses :softmax activation to return a probability distribution over the 10 labels [0 - 9].","ref":"mnist.html#defining-the-model","title":"MNIST - Defining the Model","type":"extras"},{"doc":"Axon boils the task of training down to defining a training step and passing the step to a training loop. You can use Axon.Training.step/3 to create a generic training step with a model, a loss function, and an optimizer. In this example, we'll use categorical cross-entropy and the Adam optimizer. You can then pass this to a training loop with your training data to train the final model. Axon.Training.train/4 lets you specify some additional options as well, such as the Nx compiler to use. In this example we'll train for 10 epochs using the EXLA compiler, logging metrics every 100 training steps. params = model |&gt; Axon.Loop . trainer ( :categorical_cross_entropy , :adam ) |&gt; Axon.Loop . metric ( :accuracy , &quot;Accuracy&quot; ) |&gt; Axon.Loop . run ( Stream . zip ( images , targets ) , %{ } , epochs : 10 , compiler : EXLA )","ref":"mnist.html#training","title":"MNIST - Training","type":"extras"},{"doc":"Now that we have the parameters from the training step, we can use them for predictions. For this the Axon.predict can be used. require Axon [ first_batch | _ ] = images Axon . predict ( model , params , first_batch )","ref":"mnist.html#prediction","title":"MNIST - Prediction","type":"extras"},{"doc":"","ref":"fashionmnist_autoencoder.html","title":"Fashion MNIST Autoencoder","type":"extras"},{"doc":"An autoencoder is a deep learning model which consists of two parts: encoder and decoder. The encoder compresses high dimensional data into a low dimensional representation and feeds it to the decoder. The decoder tries to recreate the original data from the low dimensional representation. Autoencoders can be used in the following problems: Dimensionality reduction Noise reduction Generative models Data augmentation Let's walk through a basic autoencoder implementation in Axon to get a better understanding of how they work in practice.","ref":"fashionmnist_autoencoder.html#introduction","title":"Fashion MNIST Autoencoder - Introduction","type":"extras"},{"doc":"First, we have to import some essential packages. Axon will be the primary tool to build and train the autoencoder. We also need to import EXLA for hardware acceleration, Nx for some basic tensor operations, and Scidata for downloading training data. Mix . install ( [ { :axon , github : &quot;elixir-nx/axon&quot; } , { :exla , &quot;~&gt; 0.2.2&quot; } , { :nx , &quot;~&gt; 0.2.1&quot; } , { :scidata , &quot;~&gt; 0.1.5&quot; } ] )","ref":"fashionmnist_autoencoder.html#imports","title":"Fashion MNIST Autoencoder - Imports","type":"extras"},{"doc":"EXLA provides JIT compilation to hardware accelerators. We'll start by telling EXLA which order of accelerators to use. If you have a specific target in mind, such as :cuda or :rocm , you can adjust the precedence to reflect that. The order of precedence here is tpu &gt; cuda &gt; rocm &gt; host : EXLA . set_as_nx_default ( [ :tpu , :cuda , :rocm , :host ] )","ref":"fashionmnist_autoencoder.html#configure-platforms-precedence","title":"Fashion MNIST Autoencoder - Configure platforms precedence","type":"extras"},{"doc":"To train and test how our model works, we use one of the most popular data set: MNIST fashion. It consists of small black and white images of clothes. Loading this data set is very simple. Just use Scidata.FashionMNIST.download() . defmodule DataSource do def get_data ( ) do Scidata.FashionMNIST . download ( ) end end","ref":"fashionmnist_autoencoder.html#downloading-data","title":"Fashion MNIST Autoencoder - Downloading Data","type":"extras"},{"doc":"The next step is image preprocessing. First, we need to load the training data into tensor using Nx.from_binary and reshape with Nx.reshape . Then normalize the data with Nx.divide and split the dataset into a list of batches with Nx.to_batched_list . defmodule Preprocessing do def transform_images ( { bin , type , shape } ) do bin |&gt; Nx . from_binary ( type ) |&gt; Nx . reshape ( { elem ( shape , 0 ) , 1 , 28 , 28 } ) |&gt; Nx . divide ( 255.0 ) |&gt; Nx . backend_copy ( ) |&gt; Nx . to_batched_list ( 32 ) end end","ref":"fashionmnist_autoencoder.html#preprocessing-of-the-images","title":"Fashion MNIST Autoencoder - Preprocessing of the images","type":"extras"},{"doc":"First we need to define the encoder and decoder. Both are one-layer neural nets. In the encoder, we start by flattening the input using Axon.flatten because initially, the input shape is {batch_size, 1, 28, 28} and we want to pass the input into a dense layer with Axon.dense . Our dense layer has only latent_dim number of neurons. The latent_dim or latent space is a compressed representation of data. Remember, we want our encoder to compress the input data into a lower-dimensional representation, so we choose a latent_dim which is less than the dimensionality of the input. Next, we pass the output of the encoder to the decoder and try to reconstruct the compressed data into its original form. Since our original input had a dimensionality of 784, we use an Axon.dense layer with 784 neurons. Because our original data was normalized to have pixel values between 0 and 1, we use a :sigmoid activation in our dense layer to squeeze output values between 0 and 1. Our original input shape was 28x28, so we use Axon.reshape to convert the flattened representation of the outputs into an image with correct the width and height. If we just bind the encoder and decoder sequentially, we'll get the desired model. This was pretty smooth, wasn't it? defmodule Autoencoder do def encoder ( x , latent_dim ) do x |&gt; Axon . flatten ( ) |&gt; Axon . dense ( latent_dim , activation : :relu ) end def decoder ( x ) do x |&gt; Axon . dense ( 784 , activation : :sigmoid ) |&gt; Axon . reshape ( { 1 , 28 , 28 } ) end def build_model ( input_shape , latent_dim ) do Axon . input ( input_shape , &quot;input&quot; ) |&gt; encoder ( latent_dim ) |&gt; decoder ( ) end end","ref":"fashionmnist_autoencoder.html#encoder-and-decoder","title":"Fashion MNIST Autoencoder - Encoder and decoder","type":"extras"},{"doc":"Finally, we can train the model. We'll use the :adam and :mean_squared_error loss with Axon.Loop.trainer . Our loss function will measure the aggregate error between pixels of original images and the model's reconstructed images. We'll also :mean_absolute_error using Axon.Loop.metric . Axon.Loop.run trains the model with the given training data. defmodule Trainer do def train_model ( model , train_images , epochs ) do model |&gt; Axon.Loop . trainer ( :mean_squared_error , :adam ) |&gt; Axon.Loop . metric ( :mean_absolute_error , &quot;Error&quot; ) |&gt; Axon.Loop . run ( Stream . zip ( train_images , train_images ) , %{ } , epochs : epochs , compiler : EXLA ) end end To better understand what is mean absolute error and mean square error let's go through an example. defmodule Example do def calculate_mse ( y_pred , y ) do y_pred |&gt; Nx . subtract ( y ) |&gt; Nx . power ( 2 ) |&gt; Nx . mean ( axes : [ - 1 ] ) |&gt; Nx . sum ( ) end def calculate_mae ( y_pred , y ) do y_pred |&gt; Nx . subtract ( y ) |&gt; Nx . abs ( ) |&gt; Nx . mean ( axes : [ - 1 ] ) |&gt; Nx . sum ( ) end def run_example do { images , _ } = DataSource . get_data ( ) train_images = Preprocessing . transform_images ( images ) shoe_image = train_images |&gt; hd ( ) |&gt; Nx . slice_axis ( 0 , 1 , 0 ) |&gt; Nx . reshape ( { 1 , 1 , 28 , 28 } ) noised_shoe_image = train_images |&gt; hd ( ) |&gt; Nx . slice_axis ( 0 , 1 , 0 ) |&gt; Nx . reshape ( { 1 , 1 , 28 , 28 } ) |&gt; Nx . add ( Nx . random_normal ( { 1 , 1 , 28 , 28 } , 0.0 , 0.05 ) ) random_image = train_images |&gt; Enum . at ( 1 ) |&gt; Nx . slice_axis ( 0 , 1 , 0 ) |&gt; Nx . reshape ( { 1 , 1 , 28 , 28 } ) same_images_mse_error = calculate_mse ( shoe_image , shoe_image ) similar_images_mse_error = calculate_mse ( shoe_image , noised_shoe_image ) different_images_mse_error = calculate_mse ( shoe_image , random_image ) same_images_mae_error = calculate_mae ( shoe_image , shoe_image ) similar_images_mae_error = calculate_mae ( shoe_image , noised_shoe_image ) different_images_mae_error = calculate_mae ( shoe_image , random_image ) %{ same : { [ mse : same_images_mse_error ] , [ mae : same_images_mae_error ] } , similar : { [ mse : similar_images_mse_error ] , [ mae : similar_images_mae_error ] } , different : { [ mse : different_images_mse_error ] , [ mae : different_images_mae_error ] } } end end Example . run_example ( ) We choose an image of a shoe as a reference. As we can see, the error for the same picture is 0 (both mae and mse). Indeed, when we have two exact copies, there is no pixel with different values, and the error is equal to zero. Noised image has a non-zero mse and mae but is much smaller than the error of two completely different pictures. This fact indicates the level of similarity between images. The small error implies decent prediction values. On the other hand, a large value of loss suggests poor quality of predictions.","ref":"fashionmnist_autoencoder.html#training-the-model","title":"Fashion MNIST Autoencoder - Training the model","type":"extras"},{"doc":"defmodule Evaluation do require Axon def run do { images , _ } = DataSource . get_data ( ) train_images = Preprocessing . transform_images ( images ) model = Autoencoder . build_model ( { nil , 1 , 28 , 28 } , 64 ) |&gt; IO . inspect ( ) model_state = Trainer . train_model ( model , train_images , 5 ) sample_image = train_images |&gt; hd ( ) |&gt; Nx . slice_axis ( 0 , 1 , 0 ) |&gt; Nx . reshape ( { 1 , 1 , 28 , 28 } ) sample_image |&gt; Nx . to_heatmap ( ) |&gt; IO . inspect ( ) model |&gt; Axon . predict ( model_state , sample_image , compiler : EXLA ) |&gt; Nx . to_heatmap ( ) |&gt; IO . inspect ( ) end end","ref":"fashionmnist_autoencoder.html#evaluation-of-the-model","title":"Fashion MNIST Autoencoder - Evaluation of the model","type":"extras"},{"doc":"As we can see, the generated image is similar to the input image. The only difference between them is the absence of a sign in the middle of the second shoe. The model treated the sign as noise and bled this into the plain shoe. To get the outputs run this code . Evaluation . run ( )","ref":"fashionmnist_autoencoder.html#results-analysis","title":"Fashion MNIST Autoencoder - Results analysis","type":"extras"},{"doc":"","ref":"multi_input_example.html","title":"Multi Input Models","type":"extras"},{"doc":"At the very beginning, we have to import essential packages. Axon will be the primary tool to build a deep learning model. Apart from that, we need to import EXLA to accelerate calculations and Nx since it gives us useful functions like Nx.to_heatmap and tensor operations. Mix . install ( [ { :axon , &quot;~&gt; 0.1.0-dev&quot; , github : &quot;elixir-nx/axon&quot; } , { :exla , &quot;~&gt; 0.2.2&quot; } , { :nx , &quot;~&gt; 0.2.1&quot; } ] )","ref":"multi_input_example.html#imports","title":"Multi Input Models - Imports","type":"extras"},{"doc":"### A &quot;single-layer&quot; perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0). The issue was overcame with deep learning methods. They can separate data with much more complicated patterns within the data. The XOR module task is to imitate a logical xor operation. First, we need to build a model. It contains an input layer with two inputs created by Axon.input and concatenated with Axon.concatenate . Then we have one hidden layer and one output layer both generated using Axon.dense . The model is a sequential neural net. In Axon, it's very convenient to create such a model. Just pipe a previous layer to the next layer using |&gt; , and that's it. defp build_model ( input_shape1 , input_shape2 ) do inp1 = Axon . input ( input_shape1 , &quot;x1&quot; ) inp2 = Axon . input ( input_shape2 , &quot;x2&quot; ) inp1 |&gt; Axon . concatenate ( inp2 ) |&gt; Axon . dense ( 8 , activation : :tanh ) |&gt; Axon . dense ( 1 , activation : :sigmoid ) end","ref":"multi_input_example.html#xor-model","title":"Multi Input Models - XOR model","type":"extras"},{"doc":"The next step is to batch the training data. However, our batch function does a little more than just splitting or preprocessing input data - it generates data and labels. Since we want to compute the logical XOR of 2 inputs, we generate 2 batches of random binary inputs, and then compute a target using Nx.logical_xor . defp batch do x1 = Nx . tensor ( for _ &lt;- 1 .. 32 , do : [ Enum . random ( 0 .. 1 ) ] ) x2 = Nx . tensor ( for _ &lt;- 1 .. 32 , do : [ Enum . random ( 0 .. 1 ) ] ) y = Nx . logical_xor ( x1 , x2 ) { { x1 , x2 } , y } end","ref":"multi_input_example.html#data","title":"Multi Input Models - Data","type":"extras"},{"doc":"Then we define a training procedure. To implement the procedure pass the parameters of the training into Axon.Loop.trainer . In our case we use binary cross entropy for loss and stochastic gradient descent as the optimizer. We use binary cross entropy because we can consider the task of computing XOR the same as a binary classification problem. We want our output to have a binary label 0 or 1 , and binary cross entropy is typically used in these cases. Then run the training process with Axon.Loop.run . defp train_model ( model , data , epochs ) do model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , :sgd ) |&gt; Axon.Loop . run ( data , %{ } , epochs : epochs , iterations : 1000 ) end","ref":"multi_input_example.html#training","title":"Multi Input Models - Training","type":"extras"},{"doc":"Finally, we can test our model for sample data. def run do model = build_model ( { nil , 1 } , { nil , 1 } ) data = Stream . repeatedly ( &amp; batch / 0 ) model_state = train_model ( model , data , 10 ) IO . inspect ( Axon . predict ( model , model_state , { Nx . tensor ( [ [ 0 ] ] ) , Nx . tensor ( [ [ 1 ] ] ) } ) ) end","ref":"multi_input_example.html#evaluation-of-the-model","title":"Multi Input Models - Evaluation of the model","type":"extras"},{"doc":"Now let's put everything in a module and try it out. defmodule XOR do require Axon defp build_model ( input_shape1 , input_shape2 ) do inp1 = Axon . input ( input_shape1 , &quot;x1&quot; ) inp2 = Axon . input ( input_shape2 , &quot;x2&quot; ) inp1 |&gt; Axon . concatenate ( inp2 ) |&gt; Axon . dense ( 8 , activation : :tanh ) |&gt; Axon . dense ( 1 , activation : :sigmoid ) end defp batch do x1 = Nx . tensor ( for _ &lt;- 1 .. 32 , do : [ Enum . random ( 0 .. 1 ) ] ) x2 = Nx . tensor ( for _ &lt;- 1 .. 32 , do : [ Enum . random ( 0 .. 1 ) ] ) y = Nx . logical_xor ( x1 , x2 ) { %{ &quot;x1&quot; =&gt; x1 , &quot;x2&quot; =&gt; x2 } , y } end defp train_model ( model , data , epochs ) do model |&gt; Axon.Loop . trainer ( :binary_cross_entropy , :sgd ) |&gt; Axon.Loop . run ( data , %{ } , epochs : epochs , iterations : 1000 ) end def run do model = build_model ( { nil , 1 } , { nil , 1 } ) data = Stream . repeatedly ( &amp; batch / 0 ) model_state = train_model ( model , data , 10 ) IO . inspect ( Axon . predict ( model , model_state , %{ &quot;x1&quot; =&gt; Nx . tensor ( [ [ 0 ] ] ) , &quot;x2&quot; =&gt; Nx . tensor ( [ [ 1 ] ] ) } ) ) end end","ref":"multi_input_example.html#everything-together","title":"Multi Input Models - Everything together","type":"extras"},{"doc":"XOR . run ( ) To improve the model performance, you can increase the number of training epochs. Link to the whole XOR module","ref":"multi_input_example.html#results","title":"Multi Input Models - Results","type":"extras"}]